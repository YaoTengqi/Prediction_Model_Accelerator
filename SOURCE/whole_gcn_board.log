Reconfigured FPGA and RPC runtime in 0.00s!
x0---------- [[ 9.43759024e-01  8.18548858e-01 -9.11387861e-01  4.55608994e-01
  -9.10669386e-01 -3.96961510e-01  7.18607783e-01 -2.00246841e-01
   8.04652423e-02  8.80870044e-01 -4.65452343e-01  8.83213222e-01
  -8.45218182e-01 -1.10748816e+00 -1.27521241e+00 -3.33954692e-01
  -2.72880971e-01 -1.76158711e-01 -3.92638445e-01  1.40902030e+00
  -8.65889847e-01  4.87120867e-01 -1.01502872e+00 -1.29518521e+00
   2.91614980e-01  2.25362134e+00 -4.12936956e-02 -5.09975195e-01
  -7.02491879e-01 -1.29504049e+00 -6.71651363e-01  9.54403937e-01]
 [-5.50112188e-01 -7.51582801e-01  2.63584286e-01  2.96513945e-01
  -3.33399177e-01 -1.14686584e+00  7.87336007e-03  1.55944562e+00
  -9.61739868e-02  6.52126610e-01 -2.37210369e+00 -7.27819453e-04
   2.06665182e+00 -1.44906357e-01  4.02942263e-02  8.92949700e-01
  -1.42646742e+00 -1.62314698e-01  1.58842027e+00  1.61234510e+00
   8.72148454e-01 -9.33038294e-01 -9.00718331e-01 -6.96391165e-01
   7.65910685e-01 -1.62633479e-01 -8.66530597e-01 -6.05053008e-01
  -7.69270599e-01  2.56596375e+00  2.04168305e-01 -1.62290883e+00]
 [-1.03221393e+00  4.00878847e-01 -6.97594583e-01  6.56913579e-01
  -2.22134322e-01 -1.55976605e+00  7.49203026e-01 -1.57241309e+00
   1.24998875e-02 -1.58168328e+00 -1.85766435e+00  2.11902928e+00
   5.73363543e-01  4.27857906e-01  1.36553204e+00 -1.22410822e+00
  -7.31355175e-02 -3.54854673e-01 -1.68895304e-01  2.30668426e+00
  -4.03969049e-01  2.60603738e+00  2.26335183e-01 -1.27789035e-01
  -4.40466762e-01 -4.07260150e-01 -1.19278216e+00  3.14133465e-01
   1.56845653e+00  4.44209874e-01 -2.09758759e+00  1.29542363e+00]
 [ 1.29189622e+00 -8.04861546e-01 -1.99325904e-01  1.15617824e+00
  -1.80101299e+00 -1.02482355e+00  1.78503186e-01 -8.65344927e-02
   1.45831394e+00 -7.40424454e-01  1.01962917e-01 -1.20166004e+00
  -9.83015060e-01 -1.69268739e+00  6.43035412e-01 -1.11872125e+00
  -8.06906044e-01  9.28967893e-01 -1.08819938e+00  9.92327988e-01
   4.57844228e-01  2.06806111e+00  5.66192508e-01  2.85663247e-01
   8.42607141e-01 -3.85495484e-01 -1.56903827e+00 -1.10062577e-01
  -8.31027091e-01 -3.23525429e-01  2.93707877e-01 -3.23007554e-01]
 [-6.13777995e-01  4.33537036e-01 -4.32601094e-01 -3.92823309e-01
  -8.92840028e-01 -8.48465636e-02 -7.82634497e-01  4.66371268e-01
  -9.75858510e-01  2.49451101e-01  8.04513097e-01  1.92683697e-01
  -8.31994951e-01  1.20155968e-01 -1.53884649e-01  1.66970396e+00
   1.10152707e-01  1.39203322e+00  6.66142225e-01  9.86604631e-01
   1.07680607e+00  1.08554518e+00  1.80819678e+00 -5.38384438e-01
  -6.79104686e-01 -3.16681981e-01  1.54451251e+00  1.00715089e+00
   6.01414680e-01  1.41106057e+00  1.32051933e+00 -1.29372343e-01]
 [-2.25149083e+00  5.55227518e-01  9.18779552e-01  2.16597736e-01
  -1.99229217e+00  4.05018836e-01  7.61761129e-01 -4.09653634e-01
  -3.11389148e-01  5.18427014e-01  2.89901525e-01 -3.35716695e-01
  -3.51673778e-04 -3.33234042e-01 -1.41581357e+00 -3.11686665e-01
  -1.32170117e+00  5.79033673e-01 -9.29711878e-01 -1.55612695e+00
   8.41038525e-01 -4.04013932e-01 -1.75862026e+00  1.37426347e-01
  -1.72174513e-01  1.32543951e-01 -2.93206006e-01 -9.62999105e-01
  -1.46847025e-01  1.38036340e-01 -8.80992830e-01  2.71428299e+00]
 [-2.40846574e-01  8.30044150e-01 -3.01719844e-01 -2.53068519e+00
  -1.25923264e+00  2.24362746e-01 -6.19105935e-01 -3.32335830e-02
  -6.36498570e-01 -2.28860593e+00  2.10154057e-01  1.05230343e+00
   1.22060156e+00  2.57557869e-01 -7.47611940e-01 -6.81048870e-01
  -8.41713250e-01 -1.44092500e+00 -8.24508131e-01  4.66053873e-01
  -7.39128709e-01 -1.31835485e+00  1.11147606e+00  1.13606811e+00
   2.39142820e-01  7.25067377e-01 -1.91782713e+00 -4.99750942e-01
   2.54016995e-01 -4.51118469e-01 -9.50365305e-01 -4.49015528e-01]
 [-7.31416643e-01 -8.10983181e-01  2.38697324e-03 -3.63421023e-01
  -7.37160385e-01  5.63405275e-01 -1.14627063e+00  1.19292772e+00
  -5.35419822e-01 -7.25696981e-01  3.02998930e-01  1.21864831e+00
  -1.18737781e+00 -1.21309841e+00  7.58151770e-01 -3.66607040e-01
  -9.86277997e-01  5.46308756e-01  1.59838483e-01  1.29634118e+00
  -8.65611255e-01  1.15768707e+00 -5.44689775e-01  1.81190205e+00
   2.18653455e-02 -4.49705273e-01  8.01203907e-01 -1.01008976e+00
   6.65518820e-01 -3.15724909e-01 -3.10603291e-01 -3.19241822e-01]
 [-2.27049780e+00 -1.27005994e-01 -2.45333028e+00  7.99076676e-01
   1.75809443e+00 -1.22398663e+00  1.43107486e+00  6.57996178e-01
  -1.28276542e-01  7.49060392e-01 -1.08680868e+00  7.22502172e-02
  -1.69848585e+00 -2.67684639e-01  7.36071050e-01 -1.23614645e+00
  -3.91828865e-01 -4.65732902e-01  2.08453107e+00 -1.65815401e+00
  -6.60105109e-01 -1.65075684e+00  6.17462993e-01  1.98204294e-01
   6.09426677e-01 -3.98678720e-01 -9.46250185e-02  5.86698890e-01
   1.40635324e+00 -1.21044433e+00 -7.22244918e-01  7.50317633e-01]
 [ 1.58129275e+00 -7.71782458e-01 -1.40617692e+00  5.05692065e-01
  -4.68225002e-01 -1.28532803e+00  1.81199741e+00 -1.51829040e+00
   2.30809617e+00  1.47961229e-01  1.97399223e+00 -6.88102126e-01
   5.78191221e-01  1.81904778e-01 -3.84650603e-02 -2.29148284e-01
  -4.88097280e-01 -4.75999355e-01  3.33517104e-01 -3.77847970e-01
  -4.59787190e-01 -9.93954003e-01 -2.85660803e-01  4.05465811e-01
  -6.37690127e-01 -2.98294306e-01  1.10560715e+00  3.92927378e-01
   7.86744133e-02 -3.71273845e-01 -2.02326179e+00 -5.27920961e-01]
 [-1.13752209e-01  1.57698452e+00 -1.12244546e+00 -3.32522184e-01
   5.02567410e-01  6.43422365e-01 -1.27341080e+00 -5.79293370e-01
   3.83071274e-01 -3.60657066e-01 -3.45038027e-01  1.17044306e+00
  -1.13725078e+00  4.32988137e-01  6.46011710e-01  4.81030136e-01
  -1.27386510e+00  1.08709955e+00 -2.91479874e+00 -6.73678398e-01
   3.37507010e-01  1.90916330e-01 -4.49820995e-01 -1.44943523e+00
   6.00238272e-05  1.37668774e-01 -1.45498550e+00 -1.34026125e-01
  -6.09650493e-01 -1.41030014e+00 -3.08062881e-01 -2.66431421e-01]
 [-2.07041383e-01  1.07193157e-01  3.33316207e-01  1.03472090e+00
  -1.02508163e+00  1.50589442e+00 -1.11004007e+00  6.89290285e-01
  -3.50777894e-01 -1.46191907e+00 -4.87707406e-01  1.06235027e+00
   8.89341474e-01 -5.86807430e-01 -1.05155599e+00 -8.69952321e-01
  -9.03040916e-02  1.03091240e+00 -3.54108244e-01 -1.42235720e+00
  -1.66291928e+00 -2.59065092e-01 -2.44785786e-01 -7.55163014e-01
   1.04165447e+00 -1.73508728e+00  2.10069537e+00  1.35429084e+00
  -1.45170724e+00  1.47959661e+00  2.01303989e-01 -1.61371744e+00]
 [-1.17973745e+00  2.03430033e+00 -4.31176603e-01 -7.37262845e-01
  -1.50957370e+00  1.48809269e-01  5.31675696e-01 -1.03238118e+00
  -7.22411454e-01  1.44447243e+00 -8.29642773e-01  2.48610660e-01
   1.92113936e-01  4.24145222e-01 -2.42556259e-01  1.61989057e+00
   1.36169344e-01 -5.19692004e-01  8.24115276e-01 -8.08877587e-01
  -4.85193849e-01 -1.01671243e+00  8.94628704e-01 -4.49275166e-01
   6.58905685e-01  4.27079201e-01  1.55022895e+00 -2.96936810e-01
  -1.92889762e+00  1.66323447e+00  7.16405153e-01 -7.42534816e-01]
 [ 1.39529419e+00 -4.91811156e-01  9.91439074e-02  2.35786498e-01
   9.01981771e-01 -8.59868705e-01  9.09839451e-01  2.28590578e-01
  -5.57567358e-01 -1.36980665e+00 -7.82933831e-01 -7.70584881e-01
   1.18829831e-01 -8.13652754e-01  4.19525772e-01  6.74924850e-01
   3.18047166e-01 -2.14373589e-01  1.52790070e+00  8.91332984e-01
   2.76704073e-01  3.57450880e-02  2.87822276e-01 -1.41042793e+00
   3.57657790e-01 -1.18612373e+00  6.14689849e-02  7.37948060e-01
   1.15851283e+00 -2.91159898e-01 -5.22299886e-01 -7.04456389e-01]
 [-5.12281001e-01 -1.96588027e+00 -4.54912990e-01  2.13897491e+00
  -1.47810495e+00 -3.04944247e-01  1.91631043e+00  6.73167050e-01
  -1.64891493e+00 -3.00485194e-01 -4.46413606e-01  1.06818187e+00
   2.89676696e-01 -3.51521194e-01 -1.77483535e+00 -9.24297988e-01
  -6.66256189e-01 -2.52203077e-01 -1.19187164e+00 -1.28832042e+00
  -1.56378672e-01  1.18318848e-01  7.17213333e-01  7.48597324e-01
   3.36727560e-01 -1.16714275e+00  1.49315131e+00 -4.74783689e-01
   1.13094437e+00 -1.86387300e+00 -1.87177420e-01 -1.14370799e+00]
 [-2.84312010e-01 -9.63853478e-01  7.26307154e-01  2.16283131e+00
   4.08584774e-01  4.34558004e-01 -5.06841600e-01 -4.98267084e-01
   9.82186139e-01  5.08107960e-01 -9.11381066e-01 -3.18978786e-01
   1.19620180e+00  1.45779800e+00 -6.42604291e-01  1.11154878e+00
   1.51958537e+00 -1.05738866e+00 -1.42295837e+00 -4.43085074e-01
   1.67286861e+00 -1.25336610e-02 -1.62219424e-02  1.36735010e+00
  -1.46248862e-01 -1.01052690e+00  1.48132578e-01  2.72303708e-02
  -1.11717176e+00 -3.59676540e-01 -2.05834419e-01  1.21435456e-01]
 [-5.43083310e-01  3.41241658e-02  4.23785970e-02  6.86659105e-03
  -2.06754848e-01 -1.31552875e-01  6.33114994e-01 -6.97618211e-03
   5.04843771e-01 -1.20277792e-01  1.11510657e-01 -5.56181550e-01
  -1.34721839e+00  1.05654359e+00  1.22516763e+00 -3.55194807e-02
   4.60833788e-01  8.87048915e-02 -5.92188478e-01  9.80080009e-01
  -2.69314766e+00 -3.42573017e-01  7.44346499e-01  1.87079537e+00
   7.75784194e-01  1.39926338e+00  1.71596035e-02 -1.92086592e-01
  -6.20459020e-01 -7.73402974e-02 -7.99769819e-01  8.22252214e-01]
 [ 3.60326082e-01  1.90214247e-01 -2.20683232e-01  1.60444999e+00
  -9.80867922e-01  1.65870023e+00  1.17170358e+00  8.80322456e-01
  -1.22115695e+00 -3.16292673e-01  1.58833966e-01  6.82017028e-01
   4.96420592e-01 -4.00569081e-01 -2.40292639e-01  8.97966921e-01
  -9.69102513e-03 -5.30167222e-01  8.25551987e-01 -6.16447151e-01
  -1.83459818e+00 -1.92460155e+00  3.98436666e-01 -5.24230719e-01
   2.43393853e-01 -3.43774222e-02 -8.00282955e-01 -3.73148412e-01
  -1.27537046e-02 -7.09925532e-01 -2.27804720e-01 -1.91814327e+00]
 [-1.91989124e+00  1.85497493e-01  1.15037954e+00 -1.11895764e+00
   4.16397125e-01  1.69052184e+00 -1.19638085e-01  4.54560071e-01
  -1.46220613e+00  1.12197721e+00  8.43667686e-02 -6.69113100e-01
  -2.18262047e-01  5.04066408e-01  1.06040764e+00  1.10464466e+00
  -1.14205360e+00 -1.45526397e+00  4.79740918e-01  4.03787494e-01
   1.91424632e+00 -1.41763163e+00 -8.90646696e-01  2.24968806e-01
  -1.46729612e+00  2.52056092e-01 -7.40886688e-01  4.64556903e-01
  -1.59726179e+00  1.81296527e+00 -1.14272368e+00  9.44563806e-01]
 [-1.09182453e+00 -8.56522322e-02  4.22867388e-01  4.08690155e-01
   6.60285652e-01  2.20046360e-02 -1.26206291e+00  2.83770591e-01
  -1.47373796e+00  4.00559366e-01 -5.02536058e-01 -1.46861410e+00
  -1.08685009e-01  4.34056342e-01  1.31696475e+00 -1.29843748e+00
  -4.66739416e-01 -1.22823052e-01 -7.17360258e-01  1.51997700e-01
   1.01971194e-01 -1.70429909e+00 -7.67992973e-01 -1.08246195e+00
   8.76157582e-01  2.16197777e+00 -2.40075660e+00 -6.31409764e-01
   3.31937432e-01 -7.00680315e-02 -3.71908695e-01  1.09631193e+00]
 [-1.03514576e+00  8.49345624e-02 -4.74158019e-01 -5.11433542e-01
   1.47051886e-01 -1.31266797e+00 -5.86275309e-02 -3.35873187e-01
  -2.35615063e+00  7.01750100e-01 -3.62263501e-01 -1.59261918e+00
   1.28089023e+00 -2.79864764e+00 -1.82962656e+00  9.14733931e-02
  -1.59127280e-01  1.14861226e+00  3.20884019e-01 -8.91598225e-01
   5.88055074e-01  1.94577658e+00  7.51986265e-01 -1.40242726e-01
  -8.46828043e-01 -5.85067809e-01  1.54185390e+00  6.56056926e-02
   2.97075212e-01 -2.84589946e-01 -1.20182049e+00  1.40710437e+00]
 [-1.78964901e+00  3.14835727e-01 -3.68599445e-01  1.15004969e+00
   2.06922197e+00 -2.82856584e-01 -4.32733029e-01 -6.19935095e-01
  -9.82432663e-01 -3.24466228e-01 -5.31840026e-02 -1.60317349e+00
  -1.25864530e+00 -9.32664871e-01  4.21474099e-01  8.23750317e-01
   7.40696430e-01  1.57172573e+00  9.12977085e-02 -1.23812103e+00
  -6.66795969e-01 -5.23201764e-01 -2.19412357e-01 -4.24743533e-01
  -8.82932246e-01 -1.27778387e+00  1.50904015e-01 -1.18626058e+00
   9.64033782e-01  4.06118244e-01  5.81958532e-01  8.82396758e-01]
 [-3.40304583e-01  1.65880752e+00 -3.15703183e-01  1.08338761e+00
   3.26739311e-01 -1.07923269e+00 -5.92080832e-01  4.00397360e-01
   9.89881814e-01  9.03840244e-01 -5.35046756e-01  1.48996219e-01
   4.62842375e-01 -8.21408510e-01 -1.22747831e-01 -1.15501046e+00
  -1.51131809e+00 -3.21000576e-01  7.78625727e-01 -1.47773519e-01
   7.92197108e-01  6.16405308e-01 -1.02042985e+00 -1.14945091e-01
  -5.20135224e-01  4.00898099e-01 -1.36818677e-01 -2.29783311e-01
  -2.28169207e-02  2.16361475e+00  2.85598934e-01  1.00608468e-01]
 [ 1.64843345e+00 -1.50188780e+00  4.66418177e-01  1.06569612e+00
  -1.24629903e+00  1.46944687e-01 -5.33379674e-01 -1.07199550e+00
  -1.02368131e-01  1.92840457e-01 -3.10775459e-01 -1.18734515e+00
  -1.34330809e-01  1.39793187e-01 -6.09185517e-01 -3.91800925e-02
  -5.63892365e-01  2.69068569e-01 -1.02670178e-01  8.09857070e-01
  -2.23599419e-01 -2.09629506e-01  6.34604514e-01  2.32012287e-01
  -5.01008183e-02  2.04288259e-01 -6.04590952e-01 -7.58915246e-01
   1.23232341e+00  2.04468679e+00  1.31935284e-01  3.04237872e-01]
 [ 1.87923980e+00  9.93017554e-02 -9.15169656e-01  1.16576210e-01
  -3.19077849e-01 -2.02044874e-01  1.57743800e+00  2.13418865e+00
  -9.29069817e-01  7.30831563e-01 -3.40270609e-01  4.88393128e-01
   4.61011320e-01  7.91819811e-01  3.62459004e-01 -1.34747708e+00
   7.01300383e-01 -6.27915323e-01  2.71709502e-01  2.31454134e+00
  -2.50123334e+00  1.80451024e+00 -5.40043712e-01  1.63495839e+00
   2.64454722e-01 -4.37813729e-01  3.83101761e-01 -1.18565845e+00
  -2.36658499e-01  9.68313694e-01  6.02900922e-01 -8.30435574e-01]
 [-7.55752742e-01 -1.26883972e+00 -9.01528224e-02  8.36499035e-02
   7.72537529e-01 -2.57124603e-01 -5.48564136e-01  1.55674660e+00
   4.30713832e-01 -4.91457641e-01  1.18085349e+00 -8.26834559e-01
  -1.45431471e+00  1.16356289e+00 -1.51436710e+00 -7.89851010e-01
   1.61548305e+00 -4.65225339e-01 -5.06312847e-01 -9.67305481e-01
  -9.45006609e-01 -3.24897766e-01  3.17997098e-01  3.99830103e-01
  -1.21556344e-02  8.87111962e-01  1.43043697e+00 -6.20470285e-01
  -5.15235245e-01 -2.46642184e+00  2.19177151e+00 -4.06205148e-01]
 [ 3.95123035e-01  4.68963414e-01  1.55077314e+00 -2.35708460e-01
  -4.11948889e-01  2.93812901e-01  1.48319995e+00 -1.78623450e+00
  -8.62041950e-01  7.89771140e-01 -9.38450769e-02 -1.23829877e+00
   5.17861903e-01 -3.49886954e-01 -6.04948282e-01  8.42850626e-01
  -2.45269448e-01 -7.80769944e-01  1.45993030e+00 -2.20175862e-01
  -8.11079979e-01  4.56339478e-01 -1.35509443e+00 -1.22226107e+00
   6.38079941e-01 -1.19597399e+00  1.73379898e+00 -6.50237426e-02
   2.53410768e-02  5.13158798e-01  5.60195208e-01 -1.04858927e-01]
 [-3.51068586e-01  1.34001523e-01 -8.25552344e-02 -1.07479250e+00
   7.52268955e-02 -7.60837853e-01 -3.35548699e-01 -2.86330312e-01
  -1.90434146e+00 -8.53920057e-02 -3.01867694e-01 -4.70901668e-01
  -8.73040974e-01  5.93792200e-01 -2.03950834e+00 -9.37058032e-01
  -7.23213077e-01 -1.76314354e-01 -5.49303889e-01  1.62398148e+00
   1.52750969e+00  2.66865820e-01  2.44204760e-01 -7.13802338e-01
   2.42848158e+00 -5.47972172e-02  5.16764045e-01  1.09174323e+00
  -7.58713067e-01 -7.23370075e-01 -3.97899002e-01 -7.24564373e-01]
 [ 2.13419318e-01 -1.51711667e+00 -1.72833368e-01  1.20326102e-01
   6.44826829e-01  2.18291268e-01 -3.05544406e-01  1.78435171e+00
   1.10724330e+00 -4.30952460e-01 -1.28254974e+00  1.43622514e-03
   1.08184807e-01 -2.40584001e-01  1.26454085e-01 -8.09409320e-01
  -1.53749615e-01  2.12249494e+00  1.30623424e+00 -1.42270315e+00
   1.56610131e-01 -5.07073879e-01 -9.16236520e-01 -2.97718495e-01
  -1.19921274e-01 -3.22724283e-02 -1.13821471e+00 -1.98660642e-01
  -1.30965364e+00  1.38567948e+00 -7.50360966e-01  4.16861586e-02]
 [-1.66840029e+00  1.43552017e+00  1.10010970e+00  1.15285069e-01
   5.92233360e-01  1.10030048e-01 -1.76536754e-01 -1.33479312e-01
  -1.49645284e-01 -1.36149907e+00  1.33981064e-01 -3.18031669e-01
  -4.04158086e-01 -1.27510980e-01 -2.04629660e-01 -1.76211342e-01
  -2.49942684e+00  7.76468754e-01  7.71770533e-03  7.83163846e-01
  -4.96102840e-01 -8.06945384e-01 -1.23712397e+00  1.09864247e+00
  -1.39756393e+00 -4.14685428e-01 -3.76723886e-01  2.82248288e-01
  -6.58893526e-01 -1.51863015e+00 -5.20336390e-01  1.97907674e+00]
 [-6.85362816e-02  3.35542798e-01  6.21630810e-03 -1.05604362e+00
   1.17046630e+00 -2.06349134e+00 -5.52058101e-01 -1.26413286e-01
  -8.92208755e-01 -4.11556512e-01 -6.43727303e-01 -1.42414010e+00
   7.24957049e-01 -8.77512813e-01  7.73440719e-01 -4.45211232e-01
   4.51696366e-01  3.75038415e-01  3.82326961e-01  2.48067975e+00
  -1.45356810e+00 -4.85711843e-01 -7.56503999e-01 -2.26954842e+00
   5.92226446e-01  1.35154963e+00 -1.08081102e+00 -1.08492005e+00
  -9.12716031e-01 -1.20213711e+00  3.04537833e-01 -8.93231750e-01]
 [-6.93947554e-01 -2.33221024e-01  3.14681363e+00 -9.23489094e-01
  -1.85072613e+00  4.51431602e-01 -6.25500321e-01  1.11055970e+00
  -8.69835854e-01 -6.49801910e-01  1.19377899e+00  1.45245165e-01
   2.00500622e-01 -1.79012358e-01  2.33585262e+00 -3.98753226e-01
   1.95888865e+00  1.14527678e+00  1.69036162e+00  1.49424285e-01
   5.71385249e-02  1.40732479e+00 -9.30750549e-01 -2.35666528e-01
  -4.75843042e-01 -2.09963584e+00 -1.04854479e-02 -1.02732711e-01
   3.10568720e-01  6.30766749e-02 -8.24454367e-01  2.00992942e+00]]
x1---------- [[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0.
  0. 0. 1. 0. 0. 1. 0. 0.]
 [1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0.
  1. 1. 1. 0. 1. 0. 1. 0.]
 [1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0.
  1. 1. 1. 1. 1. 0. 0. 1.]
 [1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.
  1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.
  1. 1. 0. 1. 0. 1. 0. 1.]
 [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.
  0. 1. 1. 1. 1. 1. 0. 1.]
 [0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.
  0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.
  1. 1. 0. 0. 1. 1. 1. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0.
  0. 1. 0. 1. 1. 0. 1. 1.]
 [1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1.
  0. 0. 1. 0. 1. 1. 0. 0.]
 [1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1.
  0. 0. 1. 1. 0. 1. 0. 1.]
 [1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.
  0. 1. 0. 0. 0. 1. 0. 0.]
 [1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1.
  0. 1. 1. 1. 0. 0. 0. 1.]
 [0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1.
  1. 0. 0. 1. 0. 0. 1. 0.]
 [0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1.
  0. 0. 0. 0. 1. 1. 1. 1.]
 [1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0.
  0. 0. 0. 1. 1. 0. 1. 0.]
 [1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.
  0. 0. 0. 0. 1. 0. 1. 1.]
 [1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0.
  0. 1. 0. 1. 1. 1. 1. 1.]
 [1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0.
  1. 0. 1. 0. 0. 0. 1. 1.]
 [0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.
  1. 0. 1. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1.
  0. 1. 0. 1. 0. 0. 1. 0.]
 [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.
  0. 1. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0.
  1. 1. 0. 0. 0. 1. 1. 0.]
 [0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.
  0. 1. 1. 1. 0. 0. 1. 1.]
 [0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1.
  0. 1. 1. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 0. 0.]
 [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
  0. 0. 1. 0. 0. 1. 0. 1.]
 [0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.
  1. 1. 0. 0. 0. 0. 0. 1.]
 [0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1.
  0. 0. 1. 0. 1. 0. 0. 1.]
 [1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.
  0. 1. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
  0. 1. 1. 1. 0. 1. 1. 1.]
 [1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0.
  1. 1. 0. 0. 1. 1. 1. 1.]]
              ******************pytorch_result********************************
              torch.Size([1, 1])
              tensor([[0.0090]])
              print(model)
              [('x0', (32, 32)), ('x1', (32, 32))]
              RecursiveScriptModule(
                original_name=GCN
                (gc): RecursiveScriptModule(

    original_name=ModuleList
    (0): RecursiveScriptModule(
      original_name=GraphConvolution
      (fc2): RecursiveScriptModule(
        original_name=Linear
        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)
      )
      (matmul): RecursiveScriptModule(
        original_name=QFunctional
        (activation_post_process): RecursiveScriptModule(original_name=Identity)
      )
      (conv): RecursiveScriptModule(original_name=Conv2d)
    )
    (1): RecursiveScriptModule(
      original_name=GraphConvolution
      (fc2): RecursiveScriptModule(
        original_name=Linear
        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)
      )
      (matmul): RecursiveScriptModule(
        original_name=QFunctional
        (activation_post_process): RecursiveScriptModule(original_name=Identity)
      )
      (conv): RecursiveScriptModule(original_name=Conv2d)
    )
    (2): RecursiveScriptModule(
      original_name=GraphConvolution
      (fc2): RecursiveScriptModule(
        original_name=Linear
        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)
      )
      (matmul): RecursiveScriptModule(
        original_name=QFunctional
        (activation_post_process): RecursiveScriptModule(original_name=Identity)
      )
      (conv): RecursiveScriptModule(original_name=Conv2d)
    )
    (3): RecursiveScriptModule(
      original_name=GraphConvolution
      (fc2): RecursiveScriptModule(
        original_name=Linear
        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)
      )
      (matmul): RecursiveScriptModule(
        original_name=QFunctional
        (activation_post_process): RecursiveScriptModule(original_name=Identity)
      )
      (conv): RecursiveScriptModule(original_name=Conv2d)
    )
  )
  (bn): RecursiveScriptModule(
    original_name=ModuleList
    (0): RecursiveScriptModule(original_name=LayerNorm)
    (1): RecursiveScriptModule(original_name=LayerNorm)
    (2): RecursiveScriptModule(original_name=LayerNorm)
    (3): RecursiveScriptModule(original_name=LayerNorm)
  )
  (relu): RecursiveScriptModule(
    original_name=ModuleList
    (0): RecursiveScriptModule(original_name=ReLU)
    (1): RecursiveScriptModule(original_name=ReLU)
    (2): RecursiveScriptModule(original_name=ReLU)
    (3): RecursiveScriptModule(original_name=ReLU)
  )
  (quant1): RecursiveScriptModule(original_name=Quantize)
  (quant2): RecursiveScriptModule(original_name=Quantize)
  (dequant): RecursiveScriptModule(original_name=DeQuantize)
  (fc): RecursiveScriptModule(
    original_name=Linear
    (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)
  )
  (dropout): RecursiveScriptModule(
    original_name=ModuleList
    (0): RecursiveScriptModule(original_name=Dropout)
    (1): RecursiveScriptModule(original_name=Dropout)
    (2): RecursiveScriptModule(original_name=Dropout)
    (3): RecursiveScriptModule(original_name=Dropout)
  )
)
input_scales_for_bias {'_packed_params.4': 0.007843137718737125, '_packed_params.8': 0.039630863815546036, '_packed_params.12': 0.05780820548534393, '_packed_params.16': 0.04836883395910263, '_packed_params': 0.034059762954711914}
调整输入形状
[32, 32]
newshape: [1, 32, 32, 1]
weight_shape: (512, 32, 1, 1)
_make.layer_norm_special(data, gamma, beta, output_scale, output_zero_point, axis, epsilon)
weight_shape: (512, 512, 1, 1)
_make.layer_norm_special(data, gamma, beta, output_scale, output_zero_point, axis, epsilon)
weight_shape: (512, 512, 1, 1)
_make.layer_norm_special(data, gamma, beta, output_scale, output_zero_point, axis, epsilon)
weight_shape: (512, 512, 1, 1)
_make.layer_norm_special(data, gamma, beta, output_scale, output_zero_point, axis, epsilon)
dshape: (1, 512, 32, 1)
[CallNode(Op(maximum), [CallNode(Op(qnn.layer_norm), [CallNode(Op(nn.gcn_matmul), [CallNode(Op(qnn.quantize), [Var(x0, ty=TensorType([32, 32], float32)), Constant(0.007843138), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2afa98), []), CallNode(Op(cast), [CallNode(Op(clip), [CallNode(Op(qnn.requantize), [CallNode(Op(nn.bias_add), [CallNode(Op(qnn.conv2d), [CallNode(Op(maximum), [CallNode(Op(qnn.layer_norm), [CallNode(Op(nn.gcn_matmul), [CallNode(Op(qnn.quantize), [Var(x0, ty=TensorType([32, 32], float32)), Constant(0.007843138), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2afa98), []), CallNode(Op(cast), [CallNode(Op(clip), [CallNode(Op(qnn.requantize), [CallNode(Op(nn.bias_add), [CallNode(Op(qnn.conv2d), [CallNode(Op(maximum), [CallNode(Op(qnn.layer_norm), [CallNode(Op(nn.gcn_matmul), [CallNode(Op(qnn.quantize), [Var(x0, ty=TensorType([32, 32], float32)), Constant(0.007843138), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2afa98), []), CallNode(Op(cast), [CallNode(Op(clip), [CallNode(Op(qnn.requantize), [CallNode(Op(nn.bias_add), [CallNode(Op(qnn.conv2d), [CallNode(Op(maximum), [CallNode(Op(qnn.layer_norm), [CallNode(Op(nn.gcn_matmul), [CallNode(Op(qnn.quantize), [Var(x0, ty=TensorType([32, 32], float32)), Constant(0.007843138), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2afa98), []), CallNode(Op(cast), [CallNode(Op(clip), [CallNode(Op(qnn.requantize), [CallNode(Op(nn.bias_add), [CallNode(Op(qnn.conv2d), [CallNode(Op(reshape), [CallNode(Op(transpose), [CallNode(Op(qnn.quantize), [Var(x1, ty=TensorType([32, 32], float32)), Constant(0.007843138), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af01c118), [])], relay.attrs.TransposeAttrs(0x5595aeff8e68), [])], relay.attrs.ReshapeAttrs(0x5595af2d4668), []), CallNode(Op(qnn.quantize), [Var(gc.0.fc2._packed_params_weight, ty=TensorType([512, 32, 1, 1], float32)), Constant(0.006634252), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2b1f28), []), Constant(0), Constant(0), Constant(0.007843138), Constant(0.006634252)], relay.attrs.Conv2DAttrs(0x5595aedaf0b8), []), CallNode(Op(qnn.quantize), [Var(gc.0.fc2._packed_params_bias, ty=TensorType([512], float32)), Constant(5.2033352e-05), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2b2398), [])], relay.attrs.BiasAddAttrs(0x5595af2ca518), []), Constant(5.2033352e-05), Constant(0), Constant(0.0065921587), Constant(0)], relay.attrs.RequantizeAttrs(0x5595af2d45f8), [])], relay.attrs.ClipAttrs(0x5595af2eccd8), [])], relay.attrs.CastAttrs(0x5595af2c94c8), [])], relay.attrs.GCNMatmulAttrs(0x5595af2cff28), []), CallNode(Op(qnn.quantize), [Var(quantized::layer_norm_0.weight, ty=TensorType([512], float32)), Constant(0.039630864), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2d4c58), []), CallNode(Op(qnn.quantize), [Var(quantized::layer_norm_0.bias, ty=TensorType([512], float32)), Constant(0.039630864), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2e7e48), []), Constant(0.039630864), Constant(0)], relay.attrs.LayerNormAttrs(0x5595af2cd678), []), CallNode(Op(cast), [Constant(0)], relay.attrs.CastAttrs(0x5595af2cddd8), [])], (nullptr), []), CallNode(Op(qnn.quantize), [Var(gc.1.fc2._packed_params_weight, ty=TensorType([512, 512, 1, 1], float32)), Constant(0.0046843826), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2b1728), []), Constant(0), Constant(0), Constant(0.039630864), Constant(0.0046843826)], relay.attrs.Conv2DAttrs(0x5595af2d5148), []), CallNode(Op(qnn.quantize), [Var(gc.1.fc2._packed_params_bias, ty=TensorType([512], float32)), Constant(0.00018564613), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2b1b58), [])], relay.attrs.BiasAddAttrs(0x5595af2ec208), []), Constant(0.00018564613), Constant(0), Constant(0.25665942), Constant(0)], relay.attrs.RequantizeAttrs(0x5595af2d6618), [])], relay.attrs.ClipAttrs(0x5595af02da68), [])], relay.attrs.CastAttrs(0x5595af2cac98), [])], relay.attrs.GCNMatmulAttrs(0x5595af2ca128), []), CallNode(Op(qnn.quantize), [Var(quantized::layer_norm_1.weight, ty=TensorType([512], float32)), Constant(0.057808205), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2ce0b8), []), CallNode(Op(qnn.quantize), [Var(quantized::layer_norm_1.bias, ty=TensorType([512], float32)), Constant(0.057808205), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2d40d8), []), Constant(0.057808205), Constant(0)], relay.attrs.LayerNormAttrs(0x5595af2c9f98), []), CallNode(Op(cast), [Constant(0)], relay.attrs.CastAttrs(0x5595af2c9fe8), [])], (nullptr), []), CallNode(Op(qnn.quantize), [Var(gc.2.fc2._packed_params_weight, ty=TensorType([512, 512, 1, 1], float32)), Constant(0.005088551), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2b0ef8), []), Constant(0), Constant(0), Constant(0.057808205), Constant(0.005088551)], relay.attrs.Conv2DAttrs(0x5595af2d69a8), []), CallNode(Op(qnn.quantize), [Var(gc.2.fc2._packed_params_bias, ty=TensorType([512], float32)), Constant(0.00029416), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2b14d8), [])], relay.attrs.BiasAddAttrs(0x5595af2e50d8), []), Constant(0.00029416), Constant(0), Constant(0.2896914), Constant(0)], relay.attrs.RequantizeAttrs(0x5595af2cdcc8), [])], relay.attrs.ClipAttrs(0x5595af2d3e28), [])], relay.attrs.CastAttrs(0x5595ae1a0168), [])], relay.attrs.GCNMatmulAttrs(0x5595af0362b8), []), CallNode(Op(qnn.quantize), [Var(quantized::layer_norm_2.weight, ty=TensorType([512], float32)), Constant(0.048368834), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af037c88), []), CallNode(Op(qnn.quantize), [Var(quantized::layer_norm_2.bias, ty=TensorType([512], float32)), Constant(0.048368834), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af0344d8), []), Constant(0.048368834), Constant(0)], relay.attrs.LayerNormAttrs(0x5595af034c58), []), CallNode(Op(cast), [Constant(0)], relay.attrs.CastAttrs(0x5595af2cb188), [])], (nullptr), []), CallNode(Op(qnn.quantize), [Var(gc.3.fc2._packed_params_weight, ty=TensorType([512, 512, 1, 1], float32)), Constant(0.008394404), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2b06e8), []), Constant(0), Constant(0), Constant(0.048368834), Constant(0.008394404)], relay.attrs.Conv2DAttrs(0x5595af02d888), []), CallNode(Op(qnn.quantize), [Var(gc.3.fc2._packed_params_bias, ty=TensorType([512], float32)), Constant(0.00040602754), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2b0938), [])], relay.attrs.BiasAddAttrs(0x5595af2d3f88), []), Constant(0.00040602754), Constant(0), Constant(0.9414739), Constant(0)], relay.attrs.RequantizeAttrs(0x5595af03b7a8), [])], relay.attrs.ClipAttrs(0x5595af2cf2d8), [])], relay.attrs.CastAttrs(0x5595af2cfd58), [])], relay.attrs.GCNMatmulAttrs(0x5595af03b568), []), CallNode(Op(qnn.quantize), [Var(quantized::layer_norm_3.weight, ty=TensorType([512], float32)), Constant(0.034059763), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af2c8a88), []), CallNode(Op(qnn.quantize), [Var(quantized::layer_norm_3.bias, ty=TensorType([512], float32)), Constant(0.034059763), Constant(0)], relay.attrs.QuantizeAttrs(0x5595af035218), []), Constant(0.034059763), Constant(0)], relay.attrs.LayerNormAttrs(0x5595af2eb8e8), []), CallNode(Op(cast), [Constant(0)], relay.attrs.CastAttrs(0x5595af034d48), [])], (nullptr), []), 0, 0, 9223372036854775807, 1]
weight_shape: (1, 512, 1, 1)
after unsqueeze
free_var %x0: Tensor[(32, 32), float32] /* span=aten::quantize_per_tensor_0.x0:0:0 */;
free_var %x1: Tensor[(32, 32), float32] /* span=aten::quantize_per_tensor_1.x1:0:0 */;
%0 = qnn.quantize(%x1, 0.00784314f /* span=aten::quantize_per_tensor_1:0:0 */, 0 /* span=aten::quantize_per_tensor_1:0:0 */, out_dtype="int8", axis=1) /* span=aten::quantize_per_tensor_1:0:0 */;
%1 = transpose(%0, axes=[1, 0]) /* span=quantized::linear_0:0:0 */;
free_var %gc.0.fc2._packed_params_weight: Tensor[(512, 32, 1, 1), float32] /* span=quantized::linear_0:0:0 */;
%2 = reshape(%1, newshape=[1, 32, 32, 1]) /* span=quantized::linear_0:0:0 */;
%3 = qnn.quantize(%gc.0.fc2._packed_params_weight, 0.00663425f /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_0:0:0 */;
free_var %gc.0.fc2._packed_params_bias: Tensor[(512), float32] /* span=quantized::linear_0:0:0 */;
%4 = qnn.conv2d(%2, %3, 0 /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, 0.00784314f /* span=quantized::linear_0:0:0 */, 0.00663425f /* span=quantized::linear_0:0:0 */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_0:0:0 */;
%5 = qnn.quantize(%gc.0.fc2._packed_params_bias, 5.20334e-05f /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_0:0:0 */;
%6 = nn.bias_add(%4, %5) /* span=quantized::linear_0:0:0 */;
%7 = qnn.requantize(%6, 5.20334e-05f /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, 0.00659216f /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_0:0:0 */;
%8 = clip(%7, a_min=0f, a_max=127f) /* span=quantized::linear_0:0:0 */;
%9 = qnn.quantize(%x0, 0.00784314f /* span=aten::quantize_per_tensor_0:0:0 */, 0 /* span=aten::quantize_per_tensor_0:0:0 */, out_dtype="int8", axis=1) /* span=aten::quantize_per_tensor_0:0:0 */;
%10 = cast(%8, dtype="int8") /* span=quantized::linear_0:0:0 */;
free_var %quantized::layer_norm_0.weight: Tensor[(512), float32] /* span=quantized::layer_norm_0.weight:0:0 */;
free_var %quantized::layer_norm_0.bias: Tensor[(512), float32] /* span=quantized::layer_norm_0.bias:0:0 */;
%11 = nn.gcn_matmul(%9, %10, units=None) /* span=quantized::matmul_0:0:0 */;
%12 = qnn.quantize(%quantized::layer_norm_0.weight, 0.0396309f /* span=quantized::layer_norm_0:0:0 */, 0 /* span=quantized::layer_norm_0:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_0:0:0 */;
%13 = qnn.quantize(%quantized::layer_norm_0.bias, 0.0396309f /* span=quantized::layer_norm_0:0:0 */, 0 /* span=quantized::layer_norm_0:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_0:0:0 */;
%14 = qnn.layer_norm(%11, %12, %13, 0.0396309f /* span=quantized::layer_norm_0:0:0 */, 0 /* span=quantized::layer_norm_0:0:0 */, axis=1) /* span=quantized::layer_norm_0:0:0 */;
%15 = cast(0 /* span=aten::relu_0:0:0 */, dtype="int8") /* span=aten::relu_0:0:0 */;
free_var %gc.1.fc2._packed_params_weight: Tensor[(512, 512, 1, 1), float32] /* span=quantized::linear_1:0:0 */;
%16 = maximum(%14, %15) /* span=aten::relu_0:0:0 */;
%17 = qnn.quantize(%gc.1.fc2._packed_params_weight, 0.00468438f /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_1:0:0 */;
free_var %gc.1.fc2._packed_params_bias: Tensor[(512), float32] /* span=quantized::linear_1:0:0 */;
%18 = qnn.conv2d(%16, %17, 0 /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, 0.0396309f /* span=quantized::linear_1:0:0 */, 0.00468438f /* span=quantized::linear_1:0:0 */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_1:0:0 */;
%19 = qnn.quantize(%gc.1.fc2._packed_params_bias, 0.000185646f /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_1:0:0 */;
%20 = nn.bias_add(%18, %19) /* span=quantized::linear_1:0:0 */;
%21 = qnn.requantize(%20, 0.000185646f /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, 0.256659f /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_1:0:0 */;
%22 = clip(%21, a_min=0f, a_max=127f) /* span=quantized::linear_1:0:0 */;
%23 = cast(%22, dtype="int8") /* span=quantized::linear_1:0:0 */;
free_var %quantized::layer_norm_1.weight: Tensor[(512), float32] /* span=quantized::layer_norm_1.weight:0:0 */;
free_var %quantized::layer_norm_1.bias: Tensor[(512), float32] /* span=quantized::layer_norm_1.bias:0:0 */;
%24 = nn.gcn_matmul(%9, %23, units=None) /* span=quantized::matmul_1:0:0 */;
%25 = qnn.quantize(%quantized::layer_norm_1.weight, 0.0578082f /* span=quantized::layer_norm_1:0:0 */, 0 /* span=quantized::layer_norm_1:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_1:0:0 */;
%26 = qnn.quantize(%quantized::layer_norm_1.bias, 0.0578082f /* span=quantized::layer_norm_1:0:0 */, 0 /* span=quantized::layer_norm_1:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_1:0:0 */;
%27 = qnn.layer_norm(%24, %25, %26, 0.0578082f /* span=quantized::layer_norm_1:0:0 */, 0 /* span=quantized::layer_norm_1:0:0 */, axis=1) /* span=quantized::layer_norm_1:0:0 */;
%28 = cast(0 /* span=aten::relu_1:0:0 */, dtype="int8") /* span=aten::relu_1:0:0 */;
free_var %gc.2.fc2._packed_params_weight: Tensor[(512, 512, 1, 1), float32] /* span=quantized::linear_2:0:0 */;
%29 = maximum(%27, %28) /* span=aten::relu_1:0:0 */;
%30 = qnn.quantize(%gc.2.fc2._packed_params_weight, 0.00508855f /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_2:0:0 */;
free_var %gc.2.fc2._packed_params_bias: Tensor[(512), float32] /* span=quantized::linear_2:0:0 */;
%31 = qnn.conv2d(%29, %30, 0 /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, 0.0578082f /* span=quantized::linear_2:0:0 */, 0.00508855f /* span=quantized::linear_2:0:0 */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_2:0:0 */;
%32 = qnn.quantize(%gc.2.fc2._packed_params_bias, 0.00029416f /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_2:0:0 */;
%33 = nn.bias_add(%31, %32) /* span=quantized::linear_2:0:0 */;
%34 = qnn.requantize(%33, 0.00029416f /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, 0.289691f /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_2:0:0 */;
%35 = clip(%34, a_min=0f, a_max=127f) /* span=quantized::linear_2:0:0 */;
%36 = cast(%35, dtype="int8") /* span=quantized::linear_2:0:0 */;
free_var %quantized::layer_norm_2.weight: Tensor[(512), float32] /* span=quantized::layer_norm_2.weight:0:0 */;
free_var %quantized::layer_norm_2.bias: Tensor[(512), float32] /* span=quantized::layer_norm_2.bias:0:0 */;
%37 = nn.gcn_matmul(%9, %36, units=None) /* span=quantized::matmul_2:0:0 */;
%38 = qnn.quantize(%quantized::layer_norm_2.weight, 0.0483688f /* span=quantized::layer_norm_2:0:0 */, 0 /* span=quantized::layer_norm_2:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_2:0:0 */;
%39 = qnn.quantize(%quantized::layer_norm_2.bias, 0.0483688f /* span=quantized::layer_norm_2:0:0 */, 0 /* span=quantized::layer_norm_2:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_2:0:0 */;
%40 = qnn.layer_norm(%37, %38, %39, 0.0483688f /* span=quantized::layer_norm_2:0:0 */, 0 /* span=quantized::layer_norm_2:0:0 */, axis=1) /* span=quantized::layer_norm_2:0:0 */;
%41 = cast(0 /* span=aten::relu_2:0:0 */, dtype="int8") /* span=aten::relu_2:0:0 */;
free_var %gc.3.fc2._packed_params_weight: Tensor[(512, 512, 1, 1), float32] /* span=quantized::linear_3:0:0 */;
%42 = maximum(%40, %41) /* span=aten::relu_2:0:0 */;
%43 = qnn.quantize(%gc.3.fc2._packed_params_weight, 0.0083944f /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_3:0:0 */;
free_var %gc.3.fc2._packed_params_bias: Tensor[(512), float32] /* span=quantized::linear_3:0:0 */;
%44 = qnn.conv2d(%42, %43, 0 /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, 0.0483688f /* span=quantized::linear_3:0:0 */, 0.0083944f /* span=quantized::linear_3:0:0 */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_3:0:0 */;
%45 = qnn.quantize(%gc.3.fc2._packed_params_bias, 0.000406028f /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_3:0:0 */;
%46 = nn.bias_add(%44, %45) /* span=quantized::linear_3:0:0 */;
%47 = qnn.requantize(%46, 0.000406028f /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, 0.941474f /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_3:0:0 */;
%48 = clip(%47, a_min=0f, a_max=127f) /* span=quantized::linear_3:0:0 */;
%49 = cast(%48, dtype="int8") /* span=quantized::linear_3:0:0 */;
free_var %quantized::layer_norm_3.weight: Tensor[(512), float32] /* span=quantized::layer_norm_3.weight:0:0 */;
free_var %quantized::layer_norm_3.bias: Tensor[(512), float32] /* span=quantized::layer_norm_3.bias:0:0 */;
%50 = nn.gcn_matmul(%9, %49, units=None) /* span=quantized::matmul_3:0:0 */;
%51 = qnn.quantize(%quantized::layer_norm_3.weight, 0.0340598f /* span=quantized::layer_norm_3:0:0 */, 0 /* span=quantized::layer_norm_3:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_3:0:0 */;
%52 = qnn.quantize(%quantized::layer_norm_3.bias, 0.0340598f /* span=quantized::layer_norm_3:0:0 */, 0 /* span=quantized::layer_norm_3:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_3:0:0 */;
%53 = qnn.layer_norm(%50, %51, %52, 0.0340598f /* span=quantized::layer_norm_3:0:0 */, 0 /* span=quantized::layer_norm_3:0:0 */, axis=1) /* span=quantized::layer_norm_3:0:0 */;
%54 = cast(0 /* span=aten::relu_3:0:0 */, dtype="int8") /* span=aten::relu_3:0:0 */;
%55 = maximum(%53, %54) /* span=aten::relu_3:0:0 */;
%56 = take(%55, 0 /* span=aten::select_0:0:0 */, axis=2, mode="wrap") /* span=aten::select_0:0:0 */;
reshape(%56, newshape=[1, 512, 1, 1])
****************************************************************************************************
#[version = "0.0.5"]
type List[A] {
  Cons(A, List[A]),
  Nil,
}

type Option[A] {
  Some(A),
  None,
}

type Tree[A] {
  Rose(A, List[Tree[A]]),
}

type tensor_float16_t {
  tensor_nil_float16,
  tensor0_float16(float16),
  tensor1_float16(Tensor[(?), float16]),
  tensor2_float16(Tensor[(?, ?), float16]),
  tensor3_float16(Tensor[(?, ?, ?), float16]),
  tensor4_float16(Tensor[(?, ?, ?, ?), float16]),
  tensor5_float16(Tensor[(?, ?, ?, ?, ?), float16]),
  tensor6_float16(Tensor[(?, ?, ?, ?, ?, ?), float16]),
}

type tensor_float32_t {
  tensor_nil_float32,
  tensor0_float32(float32),
  tensor1_float32(Tensor[(?), float32]),
  tensor2_float32(Tensor[(?, ?), float32]),
  tensor3_float32(Tensor[(?, ?, ?), float32]),
  tensor4_float32(Tensor[(?, ?, ?, ?), float32]),
  tensor5_float32(Tensor[(?, ?, ?, ?, ?), float32]),
  tensor6_float32(Tensor[(?, ?, ?, ?, ?, ?), float32]),
}

type tensor_float64_t {
  tensor_nil_float64,
  tensor0_float64(float64),
  tensor1_float64(Tensor[(?), float64]),
  tensor2_float64(Tensor[(?, ?), float64]),
  tensor3_float64(Tensor[(?, ?, ?), float64]),
  tensor4_float64(Tensor[(?, ?, ?, ?), float64]),
  tensor5_float64(Tensor[(?, ?, ?, ?, ?), float64]),
  tensor6_float64(Tensor[(?, ?, ?, ?, ?, ?), float64]),
}

type tensor_int16_t {
  tensor_nil_int16,
  tensor0_int16(int16),
  tensor1_int16(Tensor[(?), int16]),
  tensor2_int16(Tensor[(?, ?), int16]),
  tensor3_int16(Tensor[(?, ?, ?), int16]),
  tensor4_int16(Tensor[(?, ?, ?, ?), int16]),
  tensor5_int16(Tensor[(?, ?, ?, ?, ?), int16]),
  tensor6_int16(Tensor[(?, ?, ?, ?, ?, ?), int16]),
}

type tensor_int32_t {
  tensor_nil_int32,
  tensor0_int32(int32),
  tensor1_int32(Tensor[(?), int32]),
  tensor2_int32(Tensor[(?, ?), int32]),
  tensor3_int32(Tensor[(?, ?, ?), int32]),
  tensor4_int32(Tensor[(?, ?, ?, ?), int32]),
  tensor5_int32(Tensor[(?, ?, ?, ?, ?), int32]),
  tensor6_int32(Tensor[(?, ?, ?, ?, ?, ?), int32]),
}

type tensor_int64_t {
  tensor_nil_int64,
  tensor0_int64(int64),
  tensor1_int64(Tensor[(?), int64]),
  tensor2_int64(Tensor[(?, ?), int64]),
  tensor3_int64(Tensor[(?, ?, ?), int64]),
  tensor4_int64(Tensor[(?, ?, ?, ?), int64]),
  tensor5_int64(Tensor[(?, ?, ?, ?, ?), int64]),
  tensor6_int64(Tensor[(?, ?, ?, ?, ?, ?), int64]),
}

type tensor_int8_t {
  tensor_nil_int8,
  tensor0_int8(int8),
  tensor1_int8(Tensor[(?), int8]),
  tensor2_int8(Tensor[(?, ?), int8]),
  tensor3_int8(Tensor[(?, ?, ?), int8]),
  tensor4_int8(Tensor[(?, ?, ?, ?), int8]),
  tensor5_int8(Tensor[(?, ?, ?, ?, ?), int8]),
  tensor6_int8(Tensor[(?, ?, ?, ?, ?, ?), int8]),
}

type tensor_uint16_t {
  tensor_nil_uint16,
  tensor0_uint16(uint16),
  tensor1_uint16(Tensor[(?), uint16]),
  tensor2_uint16(Tensor[(?, ?), uint16]),
  tensor3_uint16(Tensor[(?, ?, ?), uint16]),
  tensor4_uint16(Tensor[(?, ?, ?, ?), uint16]),
  tensor5_uint16(Tensor[(?, ?, ?, ?, ?), uint16]),
  tensor6_uint16(Tensor[(?, ?, ?, ?, ?, ?), uint16]),
}

type tensor_uint8_t {
  tensor_nil_uint8,
  tensor0_uint8(uint8),
  tensor1_uint8(Tensor[(?), uint8]),
  tensor2_uint8(Tensor[(?, ?), uint8]),
  tensor3_uint8(Tensor[(?, ?, ?), uint8]),
  tensor4_uint8(Tensor[(?, ?, ?, ?), uint8]),
  tensor5_uint8(Tensor[(?, ?, ?, ?, ?), uint8]),
  tensor6_uint8(Tensor[(?, ?, ?, ?, ?, ?), uint8]),
}

def @main(%x0: Tensor[(32, 32), float32] /* span=aten::quantize_per_tensor_0.x0:0:0 */, %x1: Tensor[(32, 32), float32] /* span=aten::quantize_per_tensor_1.x1:0:0 */, %gc.0.fc2._packed_params_weight: Tensor[(512, 32, 1, 1), float32] /* span=quantized::linear_0:0:0 */, %gc.0.fc2._packed_params_bias: Tensor[(512), float32] /* span=quantized::linear_0:0:0 */, %quantized::layer_norm_0.weight: Tensor[(512), float32] /* span=quantized::layer_norm_0.weight:0:0 */, %quantized::layer_norm_0.bias: Tensor[(512), float32] /* span=quantized::layer_norm_0.bias:0:0 */, %gc.1.fc2._packed_params_weight: Tensor[(512, 512, 1, 1), float32] /* span=quantized::linear_1:0:0 */, %gc.1.fc2._packed_params_bias: Tensor[(512), float32] /* span=quantized::linear_1:0:0 */, %quantized::layer_norm_1.weight: Tensor[(512), float32] /* span=quantized::layer_norm_1.weight:0:0 */, %quantized::layer_norm_1.bias: Tensor[(512), float32] /* span=quantized::layer_norm_1.bias:0:0 */, %gc.2.fc2._packed_params_weight: Tensor[(512, 512, 1, 1), float32] /* span=quantized::linear_2:0:0 */, %gc.2.fc2._packed_params_bias: Tensor[(512), float32] /* span=quantized::linear_2:0:0 */, %quantized::layer_norm_2.weight: Tensor[(512), float32] /* span=quantized::layer_norm_2.weight:0:0 */, %quantized::layer_norm_2.bias: Tensor[(512), float32] /* span=quantized::layer_norm_2.bias:0:0 */, %gc.3.fc2._packed_params_weight: Tensor[(512, 512, 1, 1), float32] /* span=quantized::linear_3:0:0 */, %gc.3.fc2._packed_params_bias: Tensor[(512), float32] /* span=quantized::linear_3:0:0 */, %quantized::layer_norm_3.weight: Tensor[(512), float32] /* span=quantized::layer_norm_3.weight:0:0 */, %quantized::layer_norm_3.bias: Tensor[(512), float32] /* span=quantized::layer_norm_3.bias:0:0 */, %fc._packed_params_weight: Tensor[(1, 512, 1, 1), float32] /* span=quantized::linear_4:0:0 */, %fc._packed_params_bias: Tensor[(1), float32] /* span=quantized::linear_4:0:0 */) {
  %0 = qnn.quantize(%x1, 0.00784314f /* span=aten::quantize_per_tensor_1:0:0 */, 0 /* span=aten::quantize_per_tensor_1:0:0 */, out_dtype="int8", axis=1) /* span=aten::quantize_per_tensor_1:0:0 */;
  %1 = transpose(%0, axes=[1, 0]) /* span=quantized::linear_0:0:0 */;
  %2 = reshape(%1, newshape=[1, 32, 32, 1]) /* span=quantized::linear_0:0:0 */;
  %3 = qnn.quantize(%gc.0.fc2._packed_params_weight, 0.00663425f /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_0:0:0 */;
  %4 = qnn.conv2d(%2, %3, 0 /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, 0.00784314f /* span=quantized::linear_0:0:0 */, 0.00663425f /* span=quantized::linear_0:0:0 */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_0:0:0 */;
  %5 = qnn.quantize(%gc.0.fc2._packed_params_bias, 5.20334e-05f /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_0:0:0 */;
  %6 = nn.bias_add(%4, %5) /* span=quantized::linear_0:0:0 */;
  %7 = qnn.requantize(%6, 5.20334e-05f /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, 0.00659216f /* span=quantized::linear_0:0:0 */, 0 /* span=quantized::linear_0:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_0:0:0 */;
  %8 = clip(%7, a_min=0f, a_max=127f) /* span=quantized::linear_0:0:0 */;
  %9 = qnn.quantize(%x0, 0.00784314f /* span=aten::quantize_per_tensor_0:0:0 */, 0 /* span=aten::quantize_per_tensor_0:0:0 */, out_dtype="int8", axis=1) /* span=aten::quantize_per_tensor_0:0:0 */;
  %10 = cast(%8, dtype="int8") /* span=quantized::linear_0:0:0 */;
  %11 = nn.gcn_matmul(%9, %10, units=None) /* span=quantized::matmul_0:0:0 */;
  %12 = qnn.quantize(%quantized::layer_norm_0.weight, 0.0396309f /* span=quantized::layer_norm_0:0:0 */, 0 /* span=quantized::layer_norm_0:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_0:0:0 */;
  %13 = qnn.quantize(%quantized::layer_norm_0.bias, 0.0396309f /* span=quantized::layer_norm_0:0:0 */, 0 /* span=quantized::layer_norm_0:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_0:0:0 */;
  %14 = qnn.layer_norm(%11, %12, %13, 0.0396309f /* span=quantized::layer_norm_0:0:0 */, 0 /* span=quantized::layer_norm_0:0:0 */, axis=1) /* span=quantized::layer_norm_0:0:0 */;
  %15 = cast(0 /* span=aten::relu_0:0:0 */, dtype="int8") /* span=aten::relu_0:0:0 */;
  %16 = maximum(%14, %15) /* span=aten::relu_0:0:0 */;
  %17 = qnn.quantize(%gc.1.fc2._packed_params_weight, 0.00468438f /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_1:0:0 */;
  %18 = qnn.conv2d(%16, %17, 0 /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, 0.0396309f /* span=quantized::linear_1:0:0 */, 0.00468438f /* span=quantized::linear_1:0:0 */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_1:0:0 */;
  %19 = qnn.quantize(%gc.1.fc2._packed_params_bias, 0.000185646f /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_1:0:0 */;
  %20 = nn.bias_add(%18, %19) /* span=quantized::linear_1:0:0 */;
  %21 = qnn.requantize(%20, 0.000185646f /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, 0.256659f /* span=quantized::linear_1:0:0 */, 0 /* span=quantized::linear_1:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_1:0:0 */;
  %22 = clip(%21, a_min=0f, a_max=127f) /* span=quantized::linear_1:0:0 */;
  %23 = cast(%22, dtype="int8") /* span=quantized::linear_1:0:0 */;
  %24 = nn.gcn_matmul(%9, %23, units=None) /* span=quantized::matmul_1:0:0 */;
  %25 = qnn.quantize(%quantized::layer_norm_1.weight, 0.0578082f /* span=quantized::layer_norm_1:0:0 */, 0 /* span=quantized::layer_norm_1:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_1:0:0 */;
  %26 = qnn.quantize(%quantized::layer_norm_1.bias, 0.0578082f /* span=quantized::layer_norm_1:0:0 */, 0 /* span=quantized::layer_norm_1:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_1:0:0 */;
  %27 = qnn.layer_norm(%24, %25, %26, 0.0578082f /* span=quantized::layer_norm_1:0:0 */, 0 /* span=quantized::layer_norm_1:0:0 */, axis=1) /* span=quantized::layer_norm_1:0:0 */;
  %28 = cast(0 /* span=aten::relu_1:0:0 */, dtype="int8") /* span=aten::relu_1:0:0 */;
  %29 = maximum(%27, %28) /* span=aten::relu_1:0:0 */;
  %30 = qnn.quantize(%gc.2.fc2._packed_params_weight, 0.00508855f /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_2:0:0 */;
  %31 = qnn.conv2d(%29, %30, 0 /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, 0.0578082f /* span=quantized::linear_2:0:0 */, 0.00508855f /* span=quantized::linear_2:0:0 */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_2:0:0 */;
  %32 = qnn.quantize(%gc.2.fc2._packed_params_bias, 0.00029416f /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_2:0:0 */;
  %33 = nn.bias_add(%31, %32) /* span=quantized::linear_2:0:0 */;
  %34 = qnn.requantize(%33, 0.00029416f /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, 0.289691f /* span=quantized::linear_2:0:0 */, 0 /* span=quantized::linear_2:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_2:0:0 */;
  %35 = clip(%34, a_min=0f, a_max=127f) /* span=quantized::linear_2:0:0 */;
  %36 = cast(%35, dtype="int8") /* span=quantized::linear_2:0:0 */;
  %37 = nn.gcn_matmul(%9, %36, units=None) /* span=quantized::matmul_2:0:0 */;
  %38 = qnn.quantize(%quantized::layer_norm_2.weight, 0.0483688f /* span=quantized::layer_norm_2:0:0 */, 0 /* span=quantized::layer_norm_2:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_2:0:0 */;
  %39 = qnn.quantize(%quantized::layer_norm_2.bias, 0.0483688f /* span=quantized::layer_norm_2:0:0 */, 0 /* span=quantized::layer_norm_2:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_2:0:0 */;
  %40 = qnn.layer_norm(%37, %38, %39, 0.0483688f /* span=quantized::layer_norm_2:0:0 */, 0 /* span=quantized::layer_norm_2:0:0 */, axis=1) /* span=quantized::layer_norm_2:0:0 */;
  %41 = cast(0 /* span=aten::relu_2:0:0 */, dtype="int8") /* span=aten::relu_2:0:0 */;
  %42 = maximum(%40, %41) /* span=aten::relu_2:0:0 */;
  %43 = qnn.quantize(%gc.3.fc2._packed_params_weight, 0.0083944f /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_3:0:0 */;
  %44 = qnn.conv2d(%42, %43, 0 /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, 0.0483688f /* span=quantized::linear_3:0:0 */, 0.0083944f /* span=quantized::linear_3:0:0 */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_3:0:0 */;
  %45 = qnn.quantize(%gc.3.fc2._packed_params_bias, 0.000406028f /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_3:0:0 */;
  %46 = nn.bias_add(%44, %45) /* span=quantized::linear_3:0:0 */;
  %47 = qnn.requantize(%46, 0.000406028f /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, 0.941474f /* span=quantized::linear_3:0:0 */, 0 /* span=quantized::linear_3:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_3:0:0 */;
  %48 = clip(%47, a_min=0f, a_max=127f) /* span=quantized::linear_3:0:0 */;
  %49 = cast(%48, dtype="int8") /* span=quantized::linear_3:0:0 */;
  %50 = nn.gcn_matmul(%9, %49, units=None) /* span=quantized::matmul_3:0:0 */;
  %51 = qnn.quantize(%quantized::layer_norm_3.weight, 0.0340598f /* span=quantized::layer_norm_3:0:0 */, 0 /* span=quantized::layer_norm_3:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_3:0:0 */;
  %52 = qnn.quantize(%quantized::layer_norm_3.bias, 0.0340598f /* span=quantized::layer_norm_3:0:0 */, 0 /* span=quantized::layer_norm_3:0:0 */, out_dtype="int8") /* span=quantized::layer_norm_3:0:0 */;
  %53 = qnn.layer_norm(%50, %51, %52, 0.0340598f /* span=quantized::layer_norm_3:0:0 */, 0 /* span=quantized::layer_norm_3:0:0 */, axis=1) /* span=quantized::layer_norm_3:0:0 */;
  %54 = cast(0 /* span=aten::relu_3:0:0 */, dtype="int8") /* span=aten::relu_3:0:0 */;
  %55 = maximum(%53, %54) /* span=aten::relu_3:0:0 */;
  %56 = take(%55, 0 /* span=aten::select_0:0:0 */, axis=2, mode="wrap") /* span=aten::select_0:0:0 */;
  %57 = reshape(%56, newshape=[1, 512, 1, 1]) /* span=quantized::linear_4:0:0 */;
  %58 = qnn.quantize(%fc._packed_params_weight, 0.000690677f /* span=quantized::linear_4:0:0 */, 0 /* span=quantized::linear_4:0:0 */, out_dtype="int8", axis=0) /* span=quantized::linear_4:0:0 */;
  %59 = qnn.conv2d(%57, %58, 0 /* span=quantized::linear_4:0:0 */, 0 /* span=quantized::linear_4:0:0 */, 0.0340598f /* span=quantized::linear_4:0:0 */, 0.000690677f /* span=quantized::linear_4:0:0 */, padding=[0, 0, 0, 0], channels=1, kernel_size=[1, 1], out_dtype="int32") /* span=quantized::linear_4:0:0 */;
  %60 = qnn.quantize(%fc._packed_params_bias, 2.35243e-05f /* span=quantized::linear_4:0:0 */, 0 /* span=quantized::linear_4:0:0 */, out_dtype="int32", axis=0) /* span=quantized::linear_4:0:0 */;
  %61 = nn.bias_add(%59, %60) /* span=quantized::linear_4:0:0 */;
  %62 = qnn.requantize(%61, 2.35243e-05f /* span=quantized::linear_4:0:0 */, 0 /* span=quantized::linear_4:0:0 */, 0.000376977f /* span=quantized::linear_4:0:0 */, 0 /* span=quantized::linear_4:0:0 */, axis=1, out_dtype="int8") /* span=quantized::linear_4:0:0 */;
  %63 = clip(%62, a_min=0f, a_max=127f) /* span=quantized::linear_4:0:0 */;
  %64 = cast(%63, dtype="int8") /* span=quantized::linear_4:0:0 */;
  qnn.dequantize(%64, 0.000376977f /* span=aten::dequantize_0:0:0 */, 0 /* span=aten::dequantize_0:0:0 */) /* span=aten::dequantize_0:0:0 */
}
输出原始double_multiplier
5.20334e-05 0.00659216 0.00789322
输出fixed_point_multiplier和shift
0.00789322 1084835712 6
输出原始double_multiplier
0.000185646 0.256659 0.000723317
输出fixed_point_multiplier和shift
0.000723317 1590590976 10
输出原始double_multiplier
0.00029416 0.289691 0.00101543
输出fixed_point_multiplier和shift
0.00101543 1116472064 9
输出原始double_multiplier
0.000406028 0.941474 0.000431268
输出fixed_point_multiplier和shift
0.000431268 1896736640 11
输出原始double_multiplier
2.35243e-05 0.000376977 0.0624024
输出fixed_point_multiplier和shift
0.0624024 2144130816 4
out = Add(out, ExpandBiasToMatchAxis(beta, ndim, {axis}));
out = Add(out, ExpandBiasToMatchAxis(beta, ndim, {axis}));
out = Add(out, ExpandBiasToMatchAxis(beta, ndim, {axis}));
out = Add(out, ExpandBiasToMatchAxis(beta, ndim, {axis}));

****************************************************************************************************
****************************************************************************************************
#[version = "0.0.5"]
type List[A] {
  Cons(A, List[A]),
  Nil,
}

type Option[A] {
  Some(A),
  None,
}

type Tree[A] {
  Rose(A, List[Tree[A]]),
}

type tensor_float16_t {
  tensor_nil_float16,
  tensor0_float16(float16),
  tensor1_float16(Tensor[(?), float16]),
  tensor2_float16(Tensor[(?, ?), float16]),
  tensor3_float16(Tensor[(?, ?, ?), float16]),
  tensor4_float16(Tensor[(?, ?, ?, ?), float16]),
  tensor5_float16(Tensor[(?, ?, ?, ?, ?), float16]),
  tensor6_float16(Tensor[(?, ?, ?, ?, ?, ?), float16]),
}

type tensor_float32_t {
  tensor_nil_float32,
  tensor0_float32(float32),
  tensor1_float32(Tensor[(?), float32]),
  tensor2_float32(Tensor[(?, ?), float32]),
  tensor3_float32(Tensor[(?, ?, ?), float32]),
  tensor4_float32(Tensor[(?, ?, ?, ?), float32]),
  tensor5_float32(Tensor[(?, ?, ?, ?, ?), float32]),
  tensor6_float32(Tensor[(?, ?, ?, ?, ?, ?), float32]),
}

type tensor_float64_t {
  tensor_nil_float64,
  tensor0_float64(float64),
  tensor1_float64(Tensor[(?), float64]),
  tensor2_float64(Tensor[(?, ?), float64]),
  tensor3_float64(Tensor[(?, ?, ?), float64]),
  tensor4_float64(Tensor[(?, ?, ?, ?), float64]),
  tensor5_float64(Tensor[(?, ?, ?, ?, ?), float64]),
  tensor6_float64(Tensor[(?, ?, ?, ?, ?, ?), float64]),
}

type tensor_int16_t {
  tensor_nil_int16,
  tensor0_int16(int16),
  tensor1_int16(Tensor[(?), int16]),
  tensor2_int16(Tensor[(?, ?), int16]),
  tensor3_int16(Tensor[(?, ?, ?), int16]),
  tensor4_int16(Tensor[(?, ?, ?, ?), int16]),
  tensor5_int16(Tensor[(?, ?, ?, ?, ?), int16]),
  tensor6_int16(Tensor[(?, ?, ?, ?, ?, ?), int16]),
}

type tensor_int32_t {
  tensor_nil_int32,
  tensor0_int32(int32),
  tensor1_int32(Tensor[(?), int32]),
  tensor2_int32(Tensor[(?, ?), int32]),
  tensor3_int32(Tensor[(?, ?, ?), int32]),
  tensor4_int32(Tensor[(?, ?, ?, ?), int32]),
  tensor5_int32(Tensor[(?, ?, ?, ?, ?), int32]),
  tensor6_int32(Tensor[(?, ?, ?, ?, ?, ?), int32]),
}

type tensor_int64_t {
  tensor_nil_int64,
  tensor0_int64(int64),
  tensor1_int64(Tensor[(?), int64]),
  tensor2_int64(Tensor[(?, ?), int64]),
  tensor3_int64(Tensor[(?, ?, ?), int64]),
  tensor4_int64(Tensor[(?, ?, ?, ?), int64]),
  tensor5_int64(Tensor[(?, ?, ?, ?, ?), int64]),
  tensor6_int64(Tensor[(?, ?, ?, ?, ?, ?), int64]),
}

type tensor_int8_t {
  tensor_nil_int8,
  tensor0_int8(int8),
  tensor1_int8(Tensor[(?), int8]),
  tensor2_int8(Tensor[(?, ?), int8]),
  tensor3_int8(Tensor[(?, ?, ?), int8]),
  tensor4_int8(Tensor[(?, ?, ?, ?), int8]),
  tensor5_int8(Tensor[(?, ?, ?, ?, ?), int8]),
  tensor6_int8(Tensor[(?, ?, ?, ?, ?, ?), int8]),
}

type tensor_uint16_t {
  tensor_nil_uint16,
  tensor0_uint16(uint16),
  tensor1_uint16(Tensor[(?), uint16]),
  tensor2_uint16(Tensor[(?, ?), uint16]),
  tensor3_uint16(Tensor[(?, ?, ?), uint16]),
  tensor4_uint16(Tensor[(?, ?, ?, ?), uint16]),
  tensor5_uint16(Tensor[(?, ?, ?, ?, ?), uint16]),
  tensor6_uint16(Tensor[(?, ?, ?, ?, ?, ?), uint16]),
}

type tensor_uint8_t {
  tensor_nil_uint8,
  tensor0_uint8(uint8),
  tensor1_uint8(Tensor[(?), uint8]),
  tensor2_uint8(Tensor[(?, ?), uint8]),
  tensor3_uint8(Tensor[(?, ?, ?), uint8]),
  tensor4_uint8(Tensor[(?, ?, ?, ?), uint8]),
  tensor5_uint8(Tensor[(?, ?, ?, ?, ?), uint8]),
  tensor6_uint8(Tensor[(?, ?, ?, ?, ?, ?), uint8]),
}

def @main(%x0: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_0.x0:0:0 */, %x1: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_1.x1:0:0 */, hash="e08172471d228d45", data_layout="NCHW", kernel_layout="OIHW", out_layout="") -> Tensor[(1, 1, 1, 1), float32] {
  %0 = divide(%x0, 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_0:0:0 */) /* ty=Tensor[(32, 32), float32] */;
  %1 = round(%0) /* ty=Tensor[(32, 32), float32] */;
  %2 = clip(%1, a_min=-128f, a_max=127f) /* ty=Tensor[(32, 32), float32] */;
  %3 = divide(%x1, 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_1:0:0 */) /* ty=Tensor[(32, 32), float32] */;
  %4 = round(%3) /* ty=Tensor[(32, 32), float32] */;
  %5 = clip(%4, a_min=-128f, a_max=127f) /* ty=Tensor[(32, 32), float32] */;
  %6 = cast(%5, dtype="int8") /* ty=Tensor[(32, 32), int8] */;
  %7 = transpose(%6, axes=[1, 0]) /* ty=Tensor[(32, 32), int8] */;
  %8 = reshape(%7, newshape=[1, 32, 32, 1]) /* ty=Tensor[(1, 32, 32, 1), int8] */;
  %9 = nn.conv2d(%8, meta[relay.Constant][0] /* ty=Tensor[(512, 32, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %10 = add(%9, meta[relay.Constant][1] /* ty=Tensor[(512, 1, 1), int32] */) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %11 = fixed_point_multiply(%10, multiplier=1084835712, shift=6) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %12 = clip(%11, a_min=0f, a_max=127f) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %13 = cast(%2, dtype="int8") /* ty=Tensor[(32, 32), int8] */;
  %14 = cast(%12, dtype="int8") /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %15 = nn.gcn_matmul(%13, %14, units=None) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %16 = nn.layer_norm(%15, meta[relay.Constant][2] /* ty=Tensor[(512), int8] */, meta[relay.Constant][3] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %17 = maximum(%16, meta[relay.Constant][4] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][5] /* ty=Tensor[(512, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %19 = add(%18, meta[relay.Constant][6] /* ty=Tensor[(512, 1, 1), int32] */) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %20 = fixed_point_multiply(%19, multiplier=1590590976, shift=10) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %21 = clip(%20, a_min=0f, a_max=127f) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %22 = cast(%21, dtype="int8") /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %23 = nn.gcn_matmul(%13, %22, units=None) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %24 = nn.layer_norm(%23, meta[relay.Constant][7] /* ty=Tensor[(512), int8] */, meta[relay.Constant][8] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %25 = maximum(%24, meta[relay.Constant][4] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %26 = nn.conv2d(%25, meta[relay.Constant][9] /* ty=Tensor[(512, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %27 = add(%26, meta[relay.Constant][10] /* ty=Tensor[(512, 1, 1), int32] */) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %28 = fixed_point_multiply(%27, multiplier=1116472064, shift=9) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %29 = clip(%28, a_min=0f, a_max=127f) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %30 = cast(%29, dtype="int8") /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %31 = nn.gcn_matmul(%13, %30, units=None) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %32 = nn.layer_norm(%31, meta[relay.Constant][11] /* ty=Tensor[(512), int8] */, meta[relay.Constant][12] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %33 = maximum(%32, meta[relay.Constant][4] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][13] /* ty=Tensor[(512, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %35 = add(%34, meta[relay.Constant][14] /* ty=Tensor[(512, 1, 1), int32] */) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %36 = fixed_point_multiply(%35, multiplier=1896736640, shift=11) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %37 = clip(%36, a_min=0f, a_max=127f) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %38 = cast(%37, dtype="int8") /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %39 = nn.gcn_matmul(%13, %38, units=None) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %40 = nn.layer_norm(%39, meta[relay.Constant][15] /* ty=Tensor[(512), int8] */, meta[relay.Constant][16] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %41 = maximum(%40, meta[relay.Constant][4] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %42 = take(%41, 0 /* ty=int32 span=aten::select_0:0:0 */, axis=2, mode="wrap") /* ty=Tensor[(1, 512, 1), int8] */;
  %43 = reshape(%42, newshape=[1, 512, 1, 1]) /* ty=Tensor[(1, 512, 1, 1), int8] */;
  %44 = nn.conv2d(%43, meta[relay.Constant][17] /* ty=Tensor[(1, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=1, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %45 = add(%44, meta[relay.Constant][18] /* ty=Tensor[(1, 1, 1), int32] */) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %46 = fixed_point_multiply(%45, multiplier=2144130816, shift=4) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %47 = clip(%46, a_min=0f, a_max=127f) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %48 = cast(%47, dtype="int8") /* ty=Tensor[(1, 1, 1, 1), int8] */;
  %49 = cast(%48, dtype="float32") /* ty=Tensor[(1, 1, 1, 1), float32] */;
  multiply(%49, 0.000376977f /* ty=float32 span=aten::dequantize_0:0:0 */) /* ty=Tensor[(1, 1, 1, 1), float32] */
}

/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
****************************************************************************************************
fn (%x0: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_0.x0:0:0 */, %x1: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_1.x1:0:0 */, hash="e08172471d228d45", data_layout="NCHW", kernel_layout="OIHW", out_layout="") -> Tensor[(1, 1, 1, 1), float32] {
  let %x_524 = 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_0:0:0 */;
  let %x_525 = divide(%x0, %x_524);
  let %x_526 = round(%x_525);
  let %x_527 = clip(%x_526, a_min=-128f, a_max=127f);
  let %x_528 = cast(%x_527, dtype="int8");
  let %x_529 = 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_1:0:0 */;
  let %x_530 = divide(%x1, %x_529);
  let %x_531 = round(%x_530);
  let %x_532 = clip(%x_531, a_min=-128f, a_max=127f);
  let %x_533 = cast(%x_532, dtype="int8");
  let %x_534 = transpose(%x_533, axes=[1, 0]);
  let %x_535 = reshape(%x_534, newshape=[1, 32, 32, 1]);
  let %x_536 = meta[relay.Constant][0] /* ty=Tensor[(512, 32, 1, 1), int8] */;
  let %x_537 = nn.conv2d(%x_535, %x_536, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32");
  let %x_538 = meta[relay.Constant][1] /* ty=Tensor[(512, 1, 1), int32] */;
  let %x_539 = add(%x_537, %x_538);
  let %x_541 = fixed_point_multiply(%x_539, multiplier=1084835712, shift=6);
  let %x_541 = clip(%x_540, a_min=0f, a_max=127f);
  let %x_542 = cast(%x_541, dtype="int8");
  let %x_543 = nn.gcn_matmul(%x_528, %x_542, units=None);
  let %x_544 = meta[relay.Constant][2] /* ty=Tensor[(512), int8] */;
  let %x_545 = meta[relay.Constant][3] /* ty=Tensor[(512), int8] */;
  let %x_546 = nn.layer_norm(%x_543, %x_544, %x_545, axis=1, epsilon=0f, center=False, scale=False);
  let %x_547 = meta[relay.Constant][4] /* ty=int8 */;
  let %x_548 = maximum(%x_546, %x_547);
  let %x_549 = meta[relay.Constant][5] /* ty=Tensor[(512, 512, 1, 1), int8] */;
  let %x_550 = nn.conv2d(%x_548, %x_549, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32");
  let %x_551 = meta[relay.Constant][6] /* ty=Tensor[(512, 1, 1), int32] */;
  let %x_552 = add(%x_550, %x_551);
  let %x_553 = fixed_point_multiply(%x_552, multiplier=1590590976, shift=10);
  let %x_554 = clip(%x_553, a_min=0f, a_max=127f);
  let %x_555 = cast(%x_554, dtype="int8");
  let %x_556 = nn.gcn_matmul(%x_528, %x_555, units=None);
  let %x_557 = meta[relay.Constant][7] /* ty=Tensor[(512), int8] */;
  let %x_558 = meta[relay.Constant][8] /* ty=Tensor[(512), int8] */;
  let %x_559 = nn.layer_norm(%x_556, %x_557, %x_558, axis=1, epsilon=0f, center=False, scale=False);
  let %x_560 = maximum(%x_559, %x_547);
  let %x_561 = meta[relay.Constant][9] /* ty=Tensor[(512, 512, 1, 1), int8] */;
  let %x_562 = nn.conv2d(%x_560, %x_561, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32");
  let %x_563 = meta[relay.Constant][10] /* ty=Tensor[(512, 1, 1), int32] */;
  let %x_564 = add(%x_562, %x_563);
  let %x_565 = fixed_point_multiply(%x_564, multiplier=1116472064, shift=9);
  let %x_566 = clip(%x_565, a_min=0f, a_max=127f);
  let %x_567 = cast(%x_566, dtype="int8");
  let %x_568 = nn.gcn_matmul(%x_528, %x_567, units=None);
  let %x_569 = meta[relay.Constant][11] /* ty=Tensor[(512), int8] */;
  let %x_570 = meta[relay.Constant][12] /* ty=Tensor[(512), int8] */;
  let %x_571 = nn.layer_norm(%x_568, %x_569, %x_570, axis=1, epsilon=0f, center=False, scale=False);
  let %x_572 = maximum(%x_571, %x_547);
  let %x_573 = meta[relay.Constant][13] /* ty=Tensor[(512, 512, 1, 1), int8] */;
  let %x_574 = nn.conv2d(%x_572, %x_573, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32");
  let %x_575 = meta[relay.Constant][14] /* ty=Tensor[(512, 1, 1), int32] */;
  let %x_576 = add(%x_574, %x_575);
  let %x_577 = fixed_point_multiply(%x_576, multiplier=1896736640, shift=11);
  let %x_578 = clip(%x_577, a_min=0f, a_max=127f);
  let %x_579 = cast(%x_578, dtype="int8");
  let %x_580 = nn.gcn_matmul(%x_528, %x_579, units=None);
  let %x_581 = meta[relay.Constant][15] /* ty=Tensor[(512), int8] */;
  let %x_582 = meta[relay.Constant][16] /* ty=Tensor[(512), int8] */;
  let %x_583 = nn.layer_norm(%x_580, %x_581, %x_582, axis=1, epsilon=0f, center=False, scale=False);
  let %x_584 = maximum(%x_583, %x_547);
  let %x_585 = 0 /* ty=int32 span=aten::select_0:0:0 */;
  let %x_586 = take(%x_584, %x_585, axis=2, mode="wrap");
  let %x_587 = reshape(%x_586, newshape=[1, 512, 1, 1]);
  let %x_588 = meta[relay.Constant][17] /* ty=Tensor[(1, 512, 1, 1), int8] */;
  let %x_589 = nn.conv2d(%x_587, %x_588, padding=[0, 0, 0, 0], channels=1, kernel_size=[1, 1], out_dtype="int32");
  let %x_590 = meta[relay.Constant][18] /* ty=Tensor[(1, 1, 1), int32] */;
  let %x_591 = add(%x_589, %x_590);
  let %x_592 = fixed_point_multiply(%x_591, multiplier=2144130816, shift=4);
  let %x_593 = clip(%x_592, a_min=0f, a_max=127f);
  let %x_594 = cast(%x_593, dtype="int8");
  let %x_595 = cast(%x_594, dtype="float32");
  let %x_596 = 0.000376977f /* ty=float32 span=aten::dequantize_0:0:0 */;
  let %x_597 = multiply(%x_595, %x_596);
  %x_597
} /* ty=fn (Tensor[(32, 32), float32], Tensor[(32, 32), float32]) -> Tensor[(1, 1, 1, 1), float32] */

0 divide
1 round
2 clip
3 cast
4 divide
5 round
6 clip
7 cast
8 transpose
9 reshape
10 nn.conv2d
11 add
12 fixed_point_multiply
13 clip
14 cast
15 nn.gcn_matmul
16 nn.layer_norm
17 maximum
18 nn.conv2d
19 add
20 fixed_point_multiply
21 clip
22 cast
23 nn.gcn_matmul
24 nn.layer_norm
25 maximum
26 nn.conv2d
27 add
28 fixed_point_multiply
29 clip
30 cast
31 nn.gcn_matmul
32 nn.layer_norm
33 maximum
34 nn.conv2d
35 add
36 fixed_point_multiply
37 clip
38 cast
39 nn.gcn_matmul
40 nn.layer_norm
41 maximum
****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
fn (%x0: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_0.x0:0:0 */, %x1: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_1.x1:0:0 */, hash="e08172471d228d45", data_layout="NCHW", kernel_layout="OIHW", out_layout="") -> Tensor[(1, 1, 1, 1), float32] {
  %0 = divide(%x0, 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_0:0:0 */) /* ty=Tensor[(32, 32), float32] */;
  %1 = round(%0) /* ty=Tensor[(32, 32), float32] */;
  %2 = clip(%1, a_min=-128f, a_max=127f) /* ty=Tensor[(32, 32), float32] */;
  %3 = divide(%x1, 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_1:0:0 */) /* ty=Tensor[(32, 32), float32] */;
  %4 = round(%3) /* ty=Tensor[(32, 32), float32] */;
  %5 = clip(%4, a_min=-128f, a_max=127f) /* ty=Tensor[(32, 32), float32] */;
  %6 = cast(%5, dtype="int8") /* ty=Tensor[(32, 32), int8] */;
  %7 = transpose(%6, axes=[1, 0]) /* ty=Tensor[(32, 32), int8] */;
  %8 = reshape(%7, newshape=[1, 32, 32, 1]) /* ty=Tensor[(1, 32, 32, 1), int8] */;
  %9 = annotation.bitpack_start(%8) /* ty=Tensor[(1, 32, 32, 1), int8] */;
  %10 = nn.conv2d(%9, meta[relay.Constant][0] /* ty=Tensor[(512, 32, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %11 = add(%10, meta[relay.Constant][1] /* ty=Tensor[(512, 1, 1), int32] */) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %12 = fixed_point_multiply(%11, multiplier=1084835712, shift=6) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %13 = clip(%12, a_min=0f, a_max=127f) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %14 = cast(%2, dtype="int8") /* ty=Tensor[(32, 32), int8] */;
  %15 = cast(%13, dtype="int8") /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %16 = nn.gcn_matmul(%14, %15, units=None) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %17 = nn.layer_norm(%16, meta[relay.Constant][2] /* ty=Tensor[(512), int8] */, meta[relay.Constant][3] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %18 = maximum(%17, meta[relay.Constant][4] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %19 = nn.conv2d(%18, meta[relay.Constant][5] /* ty=Tensor[(512, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %20 = add(%19, meta[relay.Constant][6] /* ty=Tensor[(512, 1, 1), int32] */) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %21 = fixed_point_multiply(%20, multiplier=1590590976, shift=10) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %22 = clip(%21, a_min=0f, a_max=127f) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %23 = cast(%22, dtype="int8") /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %24 = nn.gcn_matmul(%14, %23, units=None) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %25 = nn.layer_norm(%24, meta[relay.Constant][7] /* ty=Tensor[(512), int8] */, meta[relay.Constant][8] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %26 = maximum(%25, meta[relay.Constant][4] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %27 = nn.conv2d(%26, meta[relay.Constant][9] /* ty=Tensor[(512, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %28 = add(%27, meta[relay.Constant][10] /* ty=Tensor[(512, 1, 1), int32] */) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %29 = fixed_point_multiply(%28, multiplier=1116472064, shift=9) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %30 = clip(%29, a_min=0f, a_max=127f) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %31 = cast(%30, dtype="int8") /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %32 = nn.gcn_matmul(%14, %31, units=None) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %33 = nn.layer_norm(%32, meta[relay.Constant][11] /* ty=Tensor[(512), int8] */, meta[relay.Constant][12] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %34 = maximum(%33, meta[relay.Constant][4] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %35 = nn.conv2d(%34, meta[relay.Constant][13] /* ty=Tensor[(512, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %36 = add(%35, meta[relay.Constant][14] /* ty=Tensor[(512, 1, 1), int32] */) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %37 = fixed_point_multiply(%36, multiplier=1896736640, shift=11) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %38 = clip(%37, a_min=0f, a_max=127f) /* ty=Tensor[(1, 512, 32, 1), int32] */;
  %39 = cast(%38, dtype="int8") /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %40 = nn.gcn_matmul(%14, %39, units=None) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %41 = nn.layer_norm(%40, meta[relay.Constant][15] /* ty=Tensor[(512), int8] */, meta[relay.Constant][16] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %42 = annotation.bitpack_end(%41) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %43 = maximum(%42, meta[relay.Constant][4] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %44 = take(%43, 0 /* ty=int32 span=aten::select_0:0:0 */, axis=2, mode="wrap") /* ty=Tensor[(1, 512, 1), int8] */;
  %45 = reshape(%44, newshape=[1, 512, 1, 1]) /* ty=Tensor[(1, 512, 1, 1), int8] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][17] /* ty=Tensor[(1, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=1, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %47 = add(%46, meta[relay.Constant][18] /* ty=Tensor[(1, 1, 1), int32] */) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %48 = fixed_point_multiply(%47, multiplier=2144130816, shift=4) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %49 = clip(%48, a_min=0f, a_max=127f) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %50 = cast(%49, dtype="int8") /* ty=Tensor[(1, 1, 1, 1), int8] */;
  %51 = cast(%50, dtype="float32") /* ty=Tensor[(1, 1, 1, 1), float32] */;
  multiply(%51, 0.000376977f /* ty=float32 span=aten::dequantize_0:0:0 */) /* ty=Tensor[(1, 1, 1, 1), float32] */
} /* ty=fn (Tensor[(32, 32), float32], Tensor[(32, 32), float32]) -> Tensor[(1, 1, 1, 1), float32] */

old_shape
(1, 512, 32, 1)
32
1 512 512 32 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
fn (%x0: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_0.x0:0:0 */, %x1: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_1.x1:0:0 */, hash="e08172471d228d45", data_layout="NCHW", kernel_layout="OIHW", out_layout="") -> Tensor[(1, 1, 1, 1), float32] {
  %0 = divide(%x0, 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_0:0:0 */);
  %1 = round(%0);
  %2 = clip(%1, a_min=-128f, a_max=127f);
  %3 = divide(%x1, 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_1:0:0 */);
  %4 = round(%3);
  %5 = clip(%4, a_min=-128f, a_max=127f);
  %6 = cast(%5, dtype="int8");
  %7 = transpose(%6, axes=[1, 0]);
  %8 = reshape(%7, newshape=[1, 32, 32, 1]);
  %9 = reshape(%8, newshape=[1, 1, 1, 32, 32, 1]);
  %10 = reshape(meta[relay.Constant][0] /* ty=Tensor[(512, 32, 1, 1), int8] */, newshape=[16, 32, 1, 32, 1, 1]);
  %11 = transpose(%9, axes=[0, 2, 4, 5, 1, 3]);
  %12 = transpose(%10, axes=[0, 2, 4, 5, 1, 3]);
  %13 = reshape(meta[relay.Constant][1] /* ty=Tensor[(512, 1, 1), int32] */, newshape=[16, 32, 1, 1, 1]);
  %14 = transpose(%13, axes=[0, 2, 3, 4, 1]);
  %15 = nn.conv2d(%11, %12, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW1n32c", kernel_layout="OIHW32o32i", out_dtype="int32");
  %16 = broadcast_to(%14, shape=[16, 1, 1, 1, 32]);
  %17 = add(%15, %16);
  %18 = fixed_point_multiply(%17, multiplier=1084835712, shift=6);
  %19 = clip(%18, a_min=0f, a_max=127f);
  %20 = cast(%19, dtype="int8");
  %21 = copy(%20);
  %22 = cast(%2, dtype="int8");
  %23 = annotation.stop_fusion(%21);
  %24 = nn.gcn_matmul(%22, %23, units=None);
  %25 = annotation.stop_fusion(%24);
  %26 = nn.layer_norm(%25, meta[relay.Constant][2] /* ty=Tensor[(512), int8] */, meta[relay.Constant][3] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False);
  %27 = reshape(meta[relay.Constant][4] /* ty=Tensor[(512, 512, 1, 1), int8] */, newshape=[16, 32, 16, 32, 1, 1]);
  %28 = annotation.stop_fusion(%26);
  %29 = transpose(%27, axes=[0, 2, 4, 5, 1, 3]);
  %30 = reshape(meta[relay.Constant][5] /* ty=Tensor[(512, 1, 1), int32] */, newshape=[16, 32, 1, 1, 1]);
  %31 = transpose(%30, axes=[0, 2, 3, 4, 1]);
  %32 = nn.conv2d(%28, %29, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW1n32c", kernel_layout="OIHW32o32i", out_dtype="int32");
  %33 = broadcast_to(%31, shape=[16, 1, 1, 1, 32]);
  %34 = add(%32, %33);
  %35 = fixed_point_multiply(%34, multiplier=1590590976, shift=10);
  %36 = clip(%35, a_min=0f, a_max=127f);
  %37 = cast(%36, dtype="int8");
  %38 = copy(%37);
  %39 = annotation.stop_fusion(%38);
  %40 = nn.gcn_matmul(%22, %39, units=None);
  %41 = annotation.stop_fusion(%40);
  %42 = nn.layer_norm(%41, meta[relay.Constant][6] /* ty=Tensor[(512), int8] */, meta[relay.Constant][7] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False);
  %43 = reshape(meta[relay.Constant][8] /* ty=Tensor[(512, 512, 1, 1), int8] */, newshape=[16, 32, 16, 32, 1, 1]);
  %44 = annotation.stop_fusion(%42);
  %45 = transpose(%43, axes=[0, 2, 4, 5, 1, 3]);
  %46 = reshape(meta[relay.Constant][9] /* ty=Tensor[(512, 1, 1), int32] */, newshape=[16, 32, 1, 1, 1]);
  %47 = transpose(%46, axes=[0, 2, 3, 4, 1]);
  %48 = nn.conv2d(%44, %45, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW1n32c", kernel_layout="OIHW32o32i", out_dtype="int32");
  %49 = broadcast_to(%47, shape=[16, 1, 1, 1, 32]);
  %50 = add(%48, %49);
  %51 = fixed_point_multiply(%50, multiplier=1116472064, shift=9);
  %52 = clip(%51, a_min=0f, a_max=127f);
  %53 = cast(%52, dtype="int8");
  %54 = copy(%53);
  %55 = annotation.stop_fusion(%54);
  %56 = nn.gcn_matmul(%22, %55, units=None);
  %57 = annotation.stop_fusion(%56);
  %58 = nn.layer_norm(%57, meta[relay.Constant][10] /* ty=Tensor[(512), int8] */, meta[relay.Constant][11] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False);
  %59 = reshape(meta[relay.Constant][12] /* ty=Tensor[(512, 512, 1, 1), int8] */, newshape=[16, 32, 16, 32, 1, 1]);
  %60 = annotation.stop_fusion(%58);
  %61 = transpose(%59, axes=[0, 2, 4, 5, 1, 3]);
  %62 = reshape(meta[relay.Constant][13] /* ty=Tensor[(512, 1, 1), int32] */, newshape=[16, 32, 1, 1, 1]);
  %63 = transpose(%62, axes=[0, 2, 3, 4, 1]);
  %64 = nn.conv2d(%60, %61, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW1n32c", kernel_layout="OIHW32o32i", out_dtype="int32");
  %65 = broadcast_to(%63, shape=[16, 1, 1, 1, 32]);
  %66 = add(%64, %65);
  %67 = fixed_point_multiply(%66, multiplier=1896736640, shift=11);
  %68 = clip(%67, a_min=0f, a_max=127f);
  %69 = cast(%68, dtype="int8");
  %70 = copy(%69);
  %71 = annotation.stop_fusion(%70);
  %72 = nn.gcn_matmul(%22, %71, units=None);
  %73 = annotation.stop_fusion(%72);
  %74 = nn.layer_norm(%73, meta[relay.Constant][14] /* ty=Tensor[(512), int8] */, meta[relay.Constant][15] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False);
  %75 = annotation.stop_fusion(%74);
  %76 = transpose(%75, axes=[0, 4, 1, 5, 2, 3]);
  %77 = reshape(%76, newshape=[1, 512, 32, 1]);
  %78 = maximum(%77, meta[relay.Constant][16] /* ty=int8 */);
  %79 = take(%78, 0 /* ty=int32 span=aten::select_0:0:0 */, axis=2, mode="wrap");
  %80 = reshape(%79, newshape=[1, 512, 1, 1]);
  %81 = nn.conv2d(%80, meta[relay.Constant][17] /* ty=Tensor[(1, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=1, kernel_size=[1, 1], out_dtype="int32");
  %82 = add(%81, meta[relay.Constant][18] /* ty=Tensor[(1, 1, 1), int32] */);
  %83 = fixed_point_multiply(%82, multiplier=2144130816, shift=4);
  %84 = clip(%83, a_min=0f, a_max=127f);
  %85 = cast(%84, dtype="int8");
  %86 = cast(%85, dtype="float32");
  multiply(%86, 0.000376977f /* ty=float32 span=aten::dequantize_0:0:0 */)
} /* ty=fn (Tensor[(32, 32), float32], Tensor[(32, 32), float32]) -> Tensor[(1, 1, 1, 1), float32] */

before  build
fn (%x0: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_0.x0:0:0 */, %x1: Tensor[(32, 32), float32] /* ty=Tensor[(32, 32), float32] span=aten::quantize_per_tensor_1.x1:0:0 */, hash="e08172471d228d45", data_layout="NCHW", kernel_layout="OIHW", out_layout="") -> Tensor[(1, 1, 1, 1), float32] {
  %0 = divide(%x0, 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_0:0:0 */) /* ty=Tensor[(32, 32), float32] */;
  %1 = round(%0) /* ty=Tensor[(32, 32), float32] */;
  %2 = clip(%1, a_min=-128f, a_max=127f) /* ty=Tensor[(32, 32), float32] */;
  %3 = divide(%x1, 0.00784314f /* ty=float32 span=aten::quantize_per_tensor_1:0:0 */) /* ty=Tensor[(32, 32), float32] */;
  %4 = round(%3) /* ty=Tensor[(32, 32), float32] */;
  %5 = clip(%4, a_min=-128f, a_max=127f) /* ty=Tensor[(32, 32), float32] */;
  %6 = cast(%5, dtype="int8") /* ty=Tensor[(32, 32), int8] */;
  %7 = transpose(%6, axes=[1, 0]) /* ty=Tensor[(32, 32), int8] */;
  %8 = reshape(%7, newshape=[1, 32, 32, 1]) /* ty=Tensor[(1, 32, 32, 1), int8] */;
  %9 = reshape(%8, newshape=[1, 1, 1, 32, 32, 1]) /* ty=Tensor[(1, 1, 1, 32, 32, 1), int8] */;
  %10 = reshape(meta[relay.Constant][0] /* ty=Tensor[(512, 32, 1, 1), int8] */, newshape=[16, 32, 1, 32, 1, 1]) /* ty=Tensor[(16, 32, 1, 32, 1, 1), int8] */;
  %11 = transpose(%9, axes=[0, 2, 4, 5, 1, 3]) /* ty=Tensor[(1, 1, 32, 1, 1, 32), int8] */;
  %12 = transpose(%10, axes=[0, 2, 4, 5, 1, 3]) /* ty=Tensor[(16, 1, 1, 1, 32, 32), int8] */;
  %13 = reshape(meta[relay.Constant][1] /* ty=Tensor[(512, 1, 1), int32] */, newshape=[16, 32, 1, 1, 1]) /* ty=Tensor[(16, 32, 1, 1, 1), int32] */;
  %14 = transpose(%13, axes=[0, 2, 3, 4, 1]) /* ty=Tensor[(16, 1, 1, 1, 32), int32] */;
  %15 = nn.conv2d(%11, %12, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW1n32c", kernel_layout="OIHW32o32i", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %16 = broadcast_to(%14, shape=[16, 1, 1, 1, 32]) /* ty=Tensor[(16, 1, 1, 1, 32), int32] */;
  %17 = add(%15, %16) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %18 = fixed_point_multiply(%17, multiplier=1084835712, shift=6) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %19 = clip(%18, a_min=0f, a_max=127f) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %20 = cast(%19, dtype="int8") /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %21 = copy(%20) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %22 = cast(%2, dtype="int8") /* ty=Tensor[(32, 32), int8] */;
  %23 = annotation.stop_fusion(%21) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %24 = nn.gcn_matmul(%22, %23, units=None) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %25 = annotation.stop_fusion(%24) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %26 = nn.layer_norm(%25, meta[relay.Constant][2] /* ty=Tensor[(512), int8] */, meta[relay.Constant][3] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %27 = reshape(meta[relay.Constant][4] /* ty=Tensor[(512, 512, 1, 1), int8] */, newshape=[16, 32, 16, 32, 1, 1]) /* ty=Tensor[(16, 32, 16, 32, 1, 1), int8] */;
  %28 = annotation.stop_fusion(%26) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %29 = transpose(%27, axes=[0, 2, 4, 5, 1, 3]) /* ty=Tensor[(16, 16, 1, 1, 32, 32), int8] */;
  %30 = reshape(meta[relay.Constant][5] /* ty=Tensor[(512, 1, 1), int32] */, newshape=[16, 32, 1, 1, 1]) /* ty=Tensor[(16, 32, 1, 1, 1), int32] */;
  %31 = transpose(%30, axes=[0, 2, 3, 4, 1]) /* ty=Tensor[(16, 1, 1, 1, 32), int32] */;
  %32 = nn.conv2d(%28, %29, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW1n32c", kernel_layout="OIHW32o32i", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %33 = broadcast_to(%31, shape=[16, 1, 1, 1, 32]) /* ty=Tensor[(16, 1, 1, 1, 32), int32] */;
  %34 = add(%32, %33) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %35 = fixed_point_multiply(%34, multiplier=1590590976, shift=10) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %36 = clip(%35, a_min=0f, a_max=127f) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %37 = cast(%36, dtype="int8") /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %38 = copy(%37) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %39 = annotation.stop_fusion(%38) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %40 = nn.gcn_matmul(%22, %39, units=None) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %41 = annotation.stop_fusion(%40) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %42 = nn.layer_norm(%41, meta[relay.Constant][6] /* ty=Tensor[(512), int8] */, meta[relay.Constant][7] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %43 = reshape(meta[relay.Constant][8] /* ty=Tensor[(512, 512, 1, 1), int8] */, newshape=[16, 32, 16, 32, 1, 1]) /* ty=Tensor[(16, 32, 16, 32, 1, 1), int8] */;
  %44 = annotation.stop_fusion(%42) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %45 = transpose(%43, axes=[0, 2, 4, 5, 1, 3]) /* ty=Tensor[(16, 16, 1, 1, 32, 32), int8] */;
  %46 = reshape(meta[relay.Constant][9] /* ty=Tensor[(512, 1, 1), int32] */, newshape=[16, 32, 1, 1, 1]) /* ty=Tensor[(16, 32, 1, 1, 1), int32] */;
  %47 = transpose(%46, axes=[0, 2, 3, 4, 1]) /* ty=Tensor[(16, 1, 1, 1, 32), int32] */;
  %48 = nn.conv2d(%44, %45, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW1n32c", kernel_layout="OIHW32o32i", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %49 = broadcast_to(%47, shape=[16, 1, 1, 1, 32]) /* ty=Tensor[(16, 1, 1, 1, 32), int32] */;
  %50 = add(%48, %49) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %51 = fixed_point_multiply(%50, multiplier=1116472064, shift=9) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %52 = clip(%51, a_min=0f, a_max=127f) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %53 = cast(%52, dtype="int8") /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %54 = copy(%53) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %55 = annotation.stop_fusion(%54) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %56 = nn.gcn_matmul(%22, %55, units=None) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %57 = annotation.stop_fusion(%56) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %58 = nn.layer_norm(%57, meta[relay.Constant][10] /* ty=Tensor[(512), int8] */, meta[relay.Constant][11] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %59 = reshape(meta[relay.Constant][12] /* ty=Tensor[(512, 512, 1, 1), int8] */, newshape=[16, 32, 16, 32, 1, 1]) /* ty=Tensor[(16, 32, 16, 32, 1, 1), int8] */;
  %60 = annotation.stop_fusion(%58) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %61 = transpose(%59, axes=[0, 2, 4, 5, 1, 3]) /* ty=Tensor[(16, 16, 1, 1, 32, 32), int8] */;
  %62 = reshape(meta[relay.Constant][13] /* ty=Tensor[(512, 1, 1), int32] */, newshape=[16, 32, 1, 1, 1]) /* ty=Tensor[(16, 32, 1, 1, 1), int32] */;
  %63 = transpose(%62, axes=[0, 2, 3, 4, 1]) /* ty=Tensor[(16, 1, 1, 1, 32), int32] */;
  %64 = nn.conv2d(%60, %61, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW1n32c", kernel_layout="OIHW32o32i", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %65 = broadcast_to(%63, shape=[16, 1, 1, 1, 32]) /* ty=Tensor[(16, 1, 1, 1, 32), int32] */;
  %66 = add(%64, %65) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %67 = fixed_point_multiply(%66, multiplier=1896736640, shift=11) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %68 = clip(%67, a_min=0f, a_max=127f) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int32] */;
  %69 = cast(%68, dtype="int8") /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %70 = copy(%69) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %71 = annotation.stop_fusion(%70) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %72 = nn.gcn_matmul(%22, %71, units=None) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %73 = annotation.stop_fusion(%72) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %74 = nn.layer_norm(%73, meta[relay.Constant][14] /* ty=Tensor[(512), int8] */, meta[relay.Constant][15] /* ty=Tensor[(512), int8] */, axis=1, epsilon=0f, center=False, scale=False) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %75 = annotation.stop_fusion(%74) /* ty=Tensor[(1, 16, 32, 1, 1, 32), int8] */;
  %76 = transpose(%75, axes=[0, 4, 1, 5, 2, 3]) /* ty=Tensor[(1, 1, 16, 32, 32, 1), int8] */;
  %77 = reshape(%76, newshape=[1, 512, 32, 1]) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %78 = maximum(%77, meta[relay.Constant][16] /* ty=int8 */) /* ty=Tensor[(1, 512, 32, 1), int8] */;
  %79 = take(%78, 0 /* ty=int32 span=aten::select_0:0:0 */, axis=2, mode="wrap") /* ty=Tensor[(1, 512, 1), int8] */;
  %80 = reshape(%79, newshape=[1, 512, 1, 1]) /* ty=Tensor[(1, 512, 1, 1), int8] */;
  %81 = nn.conv2d(%80, meta[relay.Constant][17] /* ty=Tensor[(1, 512, 1, 1), int8] */, padding=[0, 0, 0, 0], channels=1, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %82 = add(%81, meta[relay.Constant][18] /* ty=Tensor[(1, 1, 1), int32] */) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %83 = fixed_point_multiply(%82, multiplier=2144130816, shift=4) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %84 = clip(%83, a_min=0f, a_max=127f) /* ty=Tensor[(1, 1, 1, 1), int32] */;
  %85 = cast(%84, dtype="int8") /* ty=Tensor[(1, 1, 1, 1), int8] */;
  %86 = cast(%85, dtype="float32") /* ty=Tensor[(1, 1, 1, 1), float32] */;
  multiply(%86, 0.000376977f /* ty=float32 span=aten::dequantize_0:0:0 */) /* ty=Tensor[(1, 1, 1, 1), float32] */
} /* ty=fn (Tensor[(32, 32), float32], Tensor[(32, 32), float32]) -> Tensor[(1, 1, 1, 1), float32] */
out = Add(out, ExpandBiasToMatchAxis(beta, ndim, {axis}));
out = Add(out, ExpandBiasToMatchAxis(beta, ndim, {axis}));
out = Add(out, ExpandBiasToMatchAxis(beta, ndim, {axis}));
out = Add(out, ExpandBiasToMatchAxis(beta, ndim, {axis}));
输出tir.q_multiply_shift相关参数
buffer[0] + buffer_1[0] 2144130816 4
QMultiplyShift的输入
buffer[0] + buffer_1[0]
2144130816

datadata.shape:[1, 1, 32, 1, 1, 32]
dst_before:T_cast
VTAStoreConcatSpecial222222
data.shape:[32, 32]
data1.shape:[1, 16, 32, 1, 1, 32]
data:<class 'tvm.te.tensor.Tensor'>
finish
dst_before:gcn_matmul_packed
VTAStoreConcatSpecial11111
zhixing
data:<class 'tvm.te.tensor.Tensor'>
finish
dst_before:layer_norm_packed
VTAStoreConcatSpecial11111
zhixing
datadata.shape:[1, 16, 32, 1, 1, 32]
dst_before:T_cast
VTAStoreConcatSpecial222222
datadata.shape:[1, 16, 32, 1, 1, 32]
dst_before:T_cast
VTAStoreConcatSpecial222222
datadata.shape:[1, 16, 32, 1, 1, 32]
dst_before:T_cast
VTAStoreConcatSpecial222222
repeat: 1
Node Name                                                                        Ops                                                                              Time(us)     Time(%)  Shape                  Inputs  Outputs  Measurements(us)  
---------                                                                        ---                                                                              --------     -------  -----                  ------  -------  ----------------
tvmgen_default_fused_nn_gcn_matmul_2                                             tvmgen_default_fused_nn_gcn_matmul                                               1005547.115  12.44    (1, 16, 32, 1, 1, 32)  2       1        [1005547.115]     
tvmgen_default_fused_nn_gcn_matmul                                               tvmgen_default_fused_nn_gcn_matmul                                               1003807.882  12.418   (1, 16, 32, 1, 1, 32)  2       1        [1003807.882]     
tvmgen_default_fused_nn_layer_norm_1                                             tvmgen_default_fused_nn_layer_norm                                               1002964.659  12.408   (1, 16, 32, 1, 1, 32)  3       1        [1002964.659]     
tvmgen_default_fused_nn_gcn_matmul_1                                             tvmgen_default_fused_nn_gcn_matmul                                               1002765.049  12.405   (1, 16, 32, 1, 1, 32)  2       1        [1002765.049]     
tvmgen_default_fused_nn_gcn_matmul_3                                             tvmgen_default_fused_nn_gcn_matmul                                               1002541.668  12.403   (1, 16, 32, 1, 1, 32)  2       1        [1002541.668]     
tvmgen_default_fused_nn_layer_norm                                               tvmgen_default_fused_nn_layer_norm                                               1002192.814  12.398   (1, 16, 32, 1, 1, 32)  3       1        [1002192.814]     
tvmgen_default_fused_nn_layer_norm_3                                             tvmgen_default_fused_nn_layer_norm                                               1001446.364  12.389   (1, 16, 32, 1, 1, 32)  3       1        [1001446.364]     
tvmgen_default_fused_nn_layer_norm_2                                             tvmgen_default_fused_nn_layer_norm                                               1001290.189  12.387   (1, 16, 32, 1, 1, 32)  3       1        [1001290.189]     
tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_1              tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_1              22231.93     0.275    (1, 16, 32, 1, 1, 32)  3       1        [22231.93]        
tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast                tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast                14422.771    0.178    (1, 16, 32, 1, 1, 32)  3       1        [14422.771]       
tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_2              tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_2              12984.464    0.161    (1, 16, 32, 1, 1, 32)  3       1        [12984.464]       
tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_3              tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_3              10958.031    0.136    (1, 16, 32, 1, 1, 32)  3       1        [10958.031]       
tvmgen_default_fused_transpose_reshape_maximum_take_reshape                      tvmgen_default_fused_transpose_reshape_maximum_take_reshape                      32.298       0.0      (1, 512, 1, 1)         2       1        [32.298]          
tvmgen_default_fused_copy                                                        tvmgen_default_fused_copy                                                        31.932       0.0      (1, 16, 32, 1, 1, 32)  1       1        [31.932]          
tvmgen_default_fused_copy_2                                                      tvmgen_default_fused_copy                                                        31.505       0.0      (1, 16, 32, 1, 1, 32)  1       1        [31.505]          
tvmgen_default_fused_divide_round_clip_cast_transpose_reshape_transpose          tvmgen_default_fused_divide_round_clip_cast_transpose_reshape_transpose          31.477       0.0      (1, 1, 32, 1, 1, 32)   1       1        [31.477]          
tvmgen_default_fused_copy_1                                                      tvmgen_default_fused_copy                                                        31.458       0.0      (1, 16, 32, 1, 1, 32)  1       1        [31.458]          
tvmgen_default_fused_copy_3                                                      tvmgen_default_fused_copy                                                        31.345       0.0      (1, 16, 32, 1, 1, 32)  1       1        [31.345]          
tvmgen_default_fused_divide_round_clip_cast                                      tvmgen_default_fused_divide_round_clip_cast                                      30.945       0.0      (32, 32)               1       1        [30.945]          
tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_multiply  tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_multiply  6.774        0.0      (1, 1, 1, 1)           3       1        [6.774]           
Total_time                                                                       -                                                                                8083380.669  -        -                      -       -        -                 
pytorch_result.shape
torch.Size([1, 1])
tvm_output
[[0.01055536]]
pytorch_result
torch.Size([1, 1])
[[0.00904746]]
diff
[[0.00150791]]
diff>0.1
[[False]]
不正确的数量
0
1
不正确的占比： 0.0